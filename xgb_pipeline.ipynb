{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, timedelta\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "\n",
    "df_train = pd.read_csv(\n",
    "    'train.csv', usecols=[1, 2, 3, 4, 5],\n",
    "    dtype={'onpromotion': bool},\n",
    "    converters={'unit_sales': lambda u: np.log1p(\n",
    "        float(u)) if float(u) > 0 else 0},\n",
    "    parse_dates=[\"date\"],\n",
    "    skiprows=range(1, 66458909)  # 2016-01-01\n",
    ").set_index(\n",
    "    ['store_nbr', 'item_nbr', 'date'])\n",
    "\n",
    "df_train.reset_index(inplace=True)\n",
    "    \n",
    "df_test = pd.read_csv(\n",
    "    \"test.csv\", usecols=[0, 1, 2, 3, 4],\n",
    "    dtype={'onpromotion': bool},\n",
    "    parse_dates=[\"date\"]  # , date_parser=parser\n",
    ").set_index(\n",
    "    ['store_nbr', 'item_nbr', 'date']\n",
    ")\n",
    "\n",
    "df_test.reset_index(inplace=True)\n",
    "\n",
    "items = pd.read_csv(\n",
    "    \"items.csv\",\n",
    ").set_index(\"item_nbr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date           0\n",
      "dcoilwtico    76\n",
      "dtype: int64 \n",
      "\n",
      "Type :  \n",
      " date          datetime64[ns]\n",
      "dcoilwtico           float64\n",
      "dtype: object\n",
      "date          0\n",
      "dcoilwtico    0\n",
      "dtype: int64 \n",
      "\n",
      "Type :  \n",
      " date          datetime64[ns]\n",
      "dcoilwtico           float64\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEVCAYAAAAb/KWvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/H+rirNH5wu9eQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x275ae5597b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date          0\n",
      "dcoilwtico    0\n",
      "dtype: int64 \n",
      "\n",
      "Type :  \n",
      " date          datetime64[ns]\n",
      "dcoilwtico           float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from datetime import date, timedelta\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "calendar=[]\n",
    "d1=datetime.datetime.strptime('2017-01-01', '%Y-%m-%d') \n",
    "d2=datetime.datetime.strptime('2017-08-31', '%Y-%m-%d')\n",
    "delta=d2-d1\n",
    "\n",
    "for i in range(delta.days+1):\n",
    "    calendar.append(datetime.date.strftime(d1+timedelta(days=i), '%Y-%m-%d'))\n",
    "    \n",
    "calendar=pd.DataFrame({'date':calendar})\n",
    "calendar['date']  = pd.to_datetime(calendar['date'])\n",
    "\n",
    "#process the oil data\n",
    "oil = pd.read_csv('oil.csv', usecols=[0,1], parse_dates=[\"date\"] )\n",
    "#use calendar df from the hols prep \n",
    "\n",
    "oil = calendar.merge(oil, left_on='date', right_on='date', how='left')\n",
    "\n",
    "#Check how many NA\n",
    "print(oil.isnull().sum(), '\\n')\n",
    "print('Type : ', '\\n', oil.dtypes)\n",
    "\n",
    "#Check index to apply the formula\n",
    "na_index_oil = oil[oil['dcoilwtico'].isnull() == True].index.values\n",
    "\n",
    "#Define the index to use to apply the formala\n",
    "na_index_oil_plus = na_index_oil.copy()\n",
    "na_index_oil_minus = np.maximum(0, na_index_oil-1)\n",
    "\n",
    "for i in range(len(na_index_oil)):\n",
    "    k = 1\n",
    "    while (na_index_oil[min(i+k,len(na_index_oil)-1)] == na_index_oil[i]+k):\n",
    "        k += 1\n",
    "    na_index_oil_plus[i] = min(len(oil)-1, na_index_oil_plus[i] + k )\n",
    "\n",
    "#Apply the formula\n",
    "for i in range(len(na_index_oil)):\n",
    "    if (na_index_oil[i] == 0):\n",
    "        oil.loc[na_index_oil[i], 'dcoilwtico'] = oil.loc[na_index_oil_plus[i], 'dcoilwtico']\n",
    "    elif (na_index_oil[i] == len(oil)):\n",
    "        oil.loc[na_index_oil[i], 'dcoilwtico'] = oil.loc[na_index_oil_minus[i], 'dcoilwtico']\n",
    "    else:\n",
    "        oil.loc[na_index_oil[i], 'dcoilwtico'] = (oil.loc[na_index_oil_plus[i], 'dcoilwtico'] + oil.loc[na_index_oil_minus[i], 'dcoilwtico'])/ 2    \n",
    "\n",
    "oil['dcoilwtico'].ffill (inplace=True)\n",
    "oil['dcoilwtico'].bfill (inplace=True)\n",
    "print(oil.isnull().sum(), '\\n')\n",
    "print('Type : ', '\\n', oil.dtypes)\n",
    "oil[['dcoilwtico']] = MinMaxScaler().fit_transform(oil[['dcoilwtico']])\n",
    "#Plot the oil values\n",
    "oil_plot = oil['dcoilwtico'].copy()\n",
    "oil_plot.index = oil['date'].copy()\n",
    "oil_plot.plot()\n",
    "plt.show()\n",
    "print(oil.isnull().sum(), '\\n')\n",
    "print('Type : ', '\\n', oil.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "store_nbr      0\n",
      "item_nbr       0\n",
      "date           0\n",
      "unit_sales     0\n",
      "onpromotion    0\n",
      "dcoilwtico     0\n",
      "dtype: int64 \n",
      "\n",
      "Type :  \n",
      " store_nbr               int64\n",
      "item_nbr                int64\n",
      "date           datetime64[ns]\n",
      "unit_sales            float64\n",
      "onpromotion              bool\n",
      "dcoilwtico            float64\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>2017-01-01 00:00:00</th>\n",
       "      <th>2017-01-02 00:00:00</th>\n",
       "      <th>2017-01-03 00:00:00</th>\n",
       "      <th>2017-01-04 00:00:00</th>\n",
       "      <th>2017-01-05 00:00:00</th>\n",
       "      <th>2017-01-06 00:00:00</th>\n",
       "      <th>2017-01-07 00:00:00</th>\n",
       "      <th>2017-01-08 00:00:00</th>\n",
       "      <th>2017-01-09 00:00:00</th>\n",
       "      <th>2017-01-10 00:00:00</th>\n",
       "      <th>...</th>\n",
       "      <th>2017-08-06 00:00:00</th>\n",
       "      <th>2017-08-07 00:00:00</th>\n",
       "      <th>2017-08-08 00:00:00</th>\n",
       "      <th>2017-08-09 00:00:00</th>\n",
       "      <th>2017-08-10 00:00:00</th>\n",
       "      <th>2017-08-11 00:00:00</th>\n",
       "      <th>2017-08-12 00:00:00</th>\n",
       "      <th>2017-08-13 00:00:00</th>\n",
       "      <th>2017-08-14 00:00:00</th>\n",
       "      <th>2017-08-15 00:00:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>store_nbr</th>\n",
       "      <th>item_nbr</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>96995</th>\n",
       "      <td>0.823333</td>\n",
       "      <td>0.823333</td>\n",
       "      <td>0.823333</td>\n",
       "      <td>0.898333</td>\n",
       "      <td>0.940833</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.87375</td>\n",
       "      <td>0.831458</td>\n",
       "      <td>0.789167</td>\n",
       "      <td>0.695</td>\n",
       "      <td>...</td>\n",
       "      <td>0.578333</td>\n",
       "      <td>0.574167</td>\n",
       "      <td>0.549167</td>\n",
       "      <td>0.5925</td>\n",
       "      <td>0.505</td>\n",
       "      <td>0.5275</td>\n",
       "      <td>0.476667</td>\n",
       "      <td>0.45125</td>\n",
       "      <td>0.425833</td>\n",
       "      <td>0.424167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99197</th>\n",
       "      <td>0.823333</td>\n",
       "      <td>0.823333</td>\n",
       "      <td>0.823333</td>\n",
       "      <td>0.898333</td>\n",
       "      <td>0.940833</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.87375</td>\n",
       "      <td>0.831458</td>\n",
       "      <td>0.789167</td>\n",
       "      <td>0.695</td>\n",
       "      <td>...</td>\n",
       "      <td>0.578333</td>\n",
       "      <td>0.574167</td>\n",
       "      <td>0.549167</td>\n",
       "      <td>0.5925</td>\n",
       "      <td>0.505</td>\n",
       "      <td>0.5275</td>\n",
       "      <td>0.476667</td>\n",
       "      <td>0.45125</td>\n",
       "      <td>0.425833</td>\n",
       "      <td>0.424167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103520</th>\n",
       "      <td>0.823333</td>\n",
       "      <td>0.823333</td>\n",
       "      <td>0.823333</td>\n",
       "      <td>0.898333</td>\n",
       "      <td>0.940833</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.87375</td>\n",
       "      <td>0.831458</td>\n",
       "      <td>0.789167</td>\n",
       "      <td>0.695</td>\n",
       "      <td>...</td>\n",
       "      <td>0.578333</td>\n",
       "      <td>0.574167</td>\n",
       "      <td>0.549167</td>\n",
       "      <td>0.5925</td>\n",
       "      <td>0.505</td>\n",
       "      <td>0.5275</td>\n",
       "      <td>0.476667</td>\n",
       "      <td>0.45125</td>\n",
       "      <td>0.425833</td>\n",
       "      <td>0.424167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103665</th>\n",
       "      <td>0.823333</td>\n",
       "      <td>0.823333</td>\n",
       "      <td>0.823333</td>\n",
       "      <td>0.898333</td>\n",
       "      <td>0.940833</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.87375</td>\n",
       "      <td>0.831458</td>\n",
       "      <td>0.789167</td>\n",
       "      <td>0.695</td>\n",
       "      <td>...</td>\n",
       "      <td>0.578333</td>\n",
       "      <td>0.574167</td>\n",
       "      <td>0.549167</td>\n",
       "      <td>0.5925</td>\n",
       "      <td>0.505</td>\n",
       "      <td>0.5275</td>\n",
       "      <td>0.476667</td>\n",
       "      <td>0.45125</td>\n",
       "      <td>0.425833</td>\n",
       "      <td>0.424167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105574</th>\n",
       "      <td>0.823333</td>\n",
       "      <td>0.823333</td>\n",
       "      <td>0.823333</td>\n",
       "      <td>0.898333</td>\n",
       "      <td>0.940833</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.87375</td>\n",
       "      <td>0.831458</td>\n",
       "      <td>0.789167</td>\n",
       "      <td>0.695</td>\n",
       "      <td>...</td>\n",
       "      <td>0.578333</td>\n",
       "      <td>0.574167</td>\n",
       "      <td>0.549167</td>\n",
       "      <td>0.5925</td>\n",
       "      <td>0.505</td>\n",
       "      <td>0.5275</td>\n",
       "      <td>0.476667</td>\n",
       "      <td>0.45125</td>\n",
       "      <td>0.425833</td>\n",
       "      <td>0.424167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 227 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "date                2017-01-01  2017-01-02  2017-01-03  2017-01-04  \\\n",
       "store_nbr item_nbr                                                   \n",
       "1         96995       0.823333    0.823333    0.823333    0.898333   \n",
       "          99197       0.823333    0.823333    0.823333    0.898333   \n",
       "          103520      0.823333    0.823333    0.823333    0.898333   \n",
       "          103665      0.823333    0.823333    0.823333    0.898333   \n",
       "          105574      0.823333    0.823333    0.823333    0.898333   \n",
       "\n",
       "date                2017-01-05  2017-01-06  2017-01-07  2017-01-08  \\\n",
       "store_nbr item_nbr                                                   \n",
       "1         96995       0.940833    0.958333     0.87375    0.831458   \n",
       "          99197       0.940833    0.958333     0.87375    0.831458   \n",
       "          103520      0.940833    0.958333     0.87375    0.831458   \n",
       "          103665      0.940833    0.958333     0.87375    0.831458   \n",
       "          105574      0.940833    0.958333     0.87375    0.831458   \n",
       "\n",
       "date                2017-01-09  2017-01-10     ...      2017-08-06  \\\n",
       "store_nbr item_nbr                             ...                   \n",
       "1         96995       0.789167       0.695     ...        0.578333   \n",
       "          99197       0.789167       0.695     ...        0.578333   \n",
       "          103520      0.789167       0.695     ...        0.578333   \n",
       "          103665      0.789167       0.695     ...        0.578333   \n",
       "          105574      0.789167       0.695     ...        0.578333   \n",
       "\n",
       "date                2017-08-07  2017-08-08  2017-08-09  2017-08-10  \\\n",
       "store_nbr item_nbr                                                   \n",
       "1         96995       0.574167    0.549167      0.5925       0.505   \n",
       "          99197       0.574167    0.549167      0.5925       0.505   \n",
       "          103520      0.574167    0.549167      0.5925       0.505   \n",
       "          103665      0.574167    0.549167      0.5925       0.505   \n",
       "          105574      0.574167    0.549167      0.5925       0.505   \n",
       "\n",
       "date                2017-08-11  2017-08-12  2017-08-13  2017-08-14  2017-08-15  \n",
       "store_nbr item_nbr                                                              \n",
       "1         96995         0.5275    0.476667     0.45125    0.425833    0.424167  \n",
       "          99197         0.5275    0.476667     0.45125    0.425833    0.424167  \n",
       "          103520        0.5275    0.476667     0.45125    0.425833    0.424167  \n",
       "          103665        0.5275    0.476667     0.45125    0.425833    0.424167  \n",
       "          105574        0.5275    0.476667     0.45125    0.425833    0.424167  \n",
       "\n",
       "[5 rows x 227 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2017 = df_train.loc[df_train.date>=pd.datetime(2017,1,1)]\n",
    "df_2017=df_2017.merge(oil, left_on='date', right_on='date', how='left')\n",
    "df_2017_oil=df_2017[['store_nbr', 'item_nbr', 'date', 'dcoilwtico']].copy()\n",
    "print(df_2017.isnull().sum(), '\\n')\n",
    "print('Type : ', '\\n', df_2017.dtypes)\n",
    "df_oil= df_2017_oil.groupby(['store_nbr', 'item_nbr', 'date'])['dcoilwtico'].max().unstack('date')\n",
    "df_oil = df_oil.apply(lambda x:x.fillna(x.value_counts().index[0]))\n",
    "df_oil.columns = df_oil.columns.get_level_values(0)\n",
    "df_oil = df_oil.reindex(df_oil.index).fillna(False)\n",
    "df_oil.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "store_nbr     0\n",
      "item_nbr      0\n",
      "date          0\n",
      "dcoilwtico    0\n",
      "dtype: int64 \n",
      "\n",
      "Type :  \n",
      " store_nbr              int64\n",
      "item_nbr               int64\n",
      "date          datetime64[ns]\n",
      "dcoilwtico           float64\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>2017-01-01 00:00:00</th>\n",
       "      <th>2017-01-02 00:00:00</th>\n",
       "      <th>2017-01-03 00:00:00</th>\n",
       "      <th>2017-01-04 00:00:00</th>\n",
       "      <th>2017-01-05 00:00:00</th>\n",
       "      <th>2017-01-06 00:00:00</th>\n",
       "      <th>2017-01-07 00:00:00</th>\n",
       "      <th>2017-01-08 00:00:00</th>\n",
       "      <th>2017-01-09 00:00:00</th>\n",
       "      <th>2017-01-10 00:00:00</th>\n",
       "      <th>...</th>\n",
       "      <th>2017-08-22 00:00:00</th>\n",
       "      <th>2017-08-23 00:00:00</th>\n",
       "      <th>2017-08-24 00:00:00</th>\n",
       "      <th>2017-08-25 00:00:00</th>\n",
       "      <th>2017-08-26 00:00:00</th>\n",
       "      <th>2017-08-27 00:00:00</th>\n",
       "      <th>2017-08-28 00:00:00</th>\n",
       "      <th>2017-08-29 00:00:00</th>\n",
       "      <th>2017-08-30 00:00:00</th>\n",
       "      <th>2017-08-31 00:00:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>store_nbr</th>\n",
       "      <th>item_nbr</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>96995</th>\n",
       "      <td>0.823333</td>\n",
       "      <td>0.823333</td>\n",
       "      <td>0.823333</td>\n",
       "      <td>0.898333</td>\n",
       "      <td>0.940833</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.87375</td>\n",
       "      <td>0.831458</td>\n",
       "      <td>0.789167</td>\n",
       "      <td>0.695</td>\n",
       "      <td>...</td>\n",
       "      <td>0.430833</td>\n",
       "      <td>0.4975</td>\n",
       "      <td>0.396667</td>\n",
       "      <td>0.430833</td>\n",
       "      <td>0.37875</td>\n",
       "      <td>0.352708</td>\n",
       "      <td>0.326667</td>\n",
       "      <td>0.331667</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.398333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99197</th>\n",
       "      <td>0.823333</td>\n",
       "      <td>0.823333</td>\n",
       "      <td>0.823333</td>\n",
       "      <td>0.898333</td>\n",
       "      <td>0.940833</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.87375</td>\n",
       "      <td>0.831458</td>\n",
       "      <td>0.789167</td>\n",
       "      <td>0.695</td>\n",
       "      <td>...</td>\n",
       "      <td>0.430833</td>\n",
       "      <td>0.4975</td>\n",
       "      <td>0.396667</td>\n",
       "      <td>0.430833</td>\n",
       "      <td>0.37875</td>\n",
       "      <td>0.352708</td>\n",
       "      <td>0.326667</td>\n",
       "      <td>0.331667</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.398333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103520</th>\n",
       "      <td>0.823333</td>\n",
       "      <td>0.823333</td>\n",
       "      <td>0.823333</td>\n",
       "      <td>0.898333</td>\n",
       "      <td>0.940833</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.87375</td>\n",
       "      <td>0.831458</td>\n",
       "      <td>0.789167</td>\n",
       "      <td>0.695</td>\n",
       "      <td>...</td>\n",
       "      <td>0.430833</td>\n",
       "      <td>0.4975</td>\n",
       "      <td>0.396667</td>\n",
       "      <td>0.430833</td>\n",
       "      <td>0.37875</td>\n",
       "      <td>0.352708</td>\n",
       "      <td>0.326667</td>\n",
       "      <td>0.331667</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.398333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103665</th>\n",
       "      <td>0.823333</td>\n",
       "      <td>0.823333</td>\n",
       "      <td>0.823333</td>\n",
       "      <td>0.898333</td>\n",
       "      <td>0.940833</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.87375</td>\n",
       "      <td>0.831458</td>\n",
       "      <td>0.789167</td>\n",
       "      <td>0.695</td>\n",
       "      <td>...</td>\n",
       "      <td>0.430833</td>\n",
       "      <td>0.4975</td>\n",
       "      <td>0.396667</td>\n",
       "      <td>0.430833</td>\n",
       "      <td>0.37875</td>\n",
       "      <td>0.352708</td>\n",
       "      <td>0.326667</td>\n",
       "      <td>0.331667</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.398333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105574</th>\n",
       "      <td>0.823333</td>\n",
       "      <td>0.823333</td>\n",
       "      <td>0.823333</td>\n",
       "      <td>0.898333</td>\n",
       "      <td>0.940833</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.87375</td>\n",
       "      <td>0.831458</td>\n",
       "      <td>0.789167</td>\n",
       "      <td>0.695</td>\n",
       "      <td>...</td>\n",
       "      <td>0.430833</td>\n",
       "      <td>0.4975</td>\n",
       "      <td>0.396667</td>\n",
       "      <td>0.430833</td>\n",
       "      <td>0.37875</td>\n",
       "      <td>0.352708</td>\n",
       "      <td>0.326667</td>\n",
       "      <td>0.331667</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.398333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 243 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "date                2017-01-01  2017-01-02  2017-01-03  2017-01-04  \\\n",
       "store_nbr item_nbr                                                   \n",
       "1         96995       0.823333    0.823333    0.823333    0.898333   \n",
       "          99197       0.823333    0.823333    0.823333    0.898333   \n",
       "          103520      0.823333    0.823333    0.823333    0.898333   \n",
       "          103665      0.823333    0.823333    0.823333    0.898333   \n",
       "          105574      0.823333    0.823333    0.823333    0.898333   \n",
       "\n",
       "date                2017-01-05  2017-01-06  2017-01-07  2017-01-08  \\\n",
       "store_nbr item_nbr                                                   \n",
       "1         96995       0.940833    0.958333     0.87375    0.831458   \n",
       "          99197       0.940833    0.958333     0.87375    0.831458   \n",
       "          103520      0.940833    0.958333     0.87375    0.831458   \n",
       "          103665      0.940833    0.958333     0.87375    0.831458   \n",
       "          105574      0.940833    0.958333     0.87375    0.831458   \n",
       "\n",
       "date                2017-01-09  2017-01-10     ...      2017-08-22  \\\n",
       "store_nbr item_nbr                             ...                   \n",
       "1         96995       0.789167       0.695     ...        0.430833   \n",
       "          99197       0.789167       0.695     ...        0.430833   \n",
       "          103520      0.789167       0.695     ...        0.430833   \n",
       "          103665      0.789167       0.695     ...        0.430833   \n",
       "          105574      0.789167       0.695     ...        0.430833   \n",
       "\n",
       "date                2017-08-23  2017-08-24  2017-08-25  2017-08-26  \\\n",
       "store_nbr item_nbr                                                   \n",
       "1         96995         0.4975    0.396667    0.430833     0.37875   \n",
       "          99197         0.4975    0.396667    0.430833     0.37875   \n",
       "          103520        0.4975    0.396667    0.430833     0.37875   \n",
       "          103665        0.4975    0.396667    0.430833     0.37875   \n",
       "          105574        0.4975    0.396667    0.430833     0.37875   \n",
       "\n",
       "date                2017-08-27  2017-08-28  2017-08-29  2017-08-30  2017-08-31  \n",
       "store_nbr item_nbr                                                              \n",
       "1         96995       0.352708    0.326667    0.331667        0.29    0.398333  \n",
       "          99197       0.352708    0.326667    0.331667        0.29    0.398333  \n",
       "          103520      0.352708    0.326667    0.331667        0.29    0.398333  \n",
       "          103665      0.352708    0.326667    0.331667        0.29    0.398333  \n",
       "          105574      0.352708    0.326667    0.331667        0.29    0.398333  \n",
       "\n",
       "[5 rows x 243 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv(\n",
    "    \"test.csv\", usecols=[0, 1, 2, 3, 4],\n",
    "    dtype={'onpromotion': bool},\n",
    "    parse_dates=[\"date\"]  # , date_parser=parser\n",
    ").set_index(\n",
    "    ['store_nbr', 'item_nbr', 'date']\n",
    ")\n",
    "\n",
    "df_test.reset_index(inplace=True)\n",
    "df_test=df_test.merge(oil, left_on='date', right_on='date', how='left')\n",
    "df_oil_test=df_test[['store_nbr', 'item_nbr', 'date', 'dcoilwtico']].copy()\n",
    "print(df_oil_test.isnull().sum(), '\\n')\n",
    "print('Type : ', '\\n', df_oil_test.dtypes)\n",
    "df_oil_test1= df_oil_test.groupby(['store_nbr', 'item_nbr', 'date'])['dcoilwtico'].max().unstack('date')\n",
    "df_oil_test1.head()\n",
    "df_oil_test1.columns = df_oil_test1.columns.get_level_values(0)\n",
    "df_oil_test1 = df_oil_test1.reindex(df_oil.index).fillna(False)\n",
    "df_oil_test1.head()\n",
    "oil_2017 = pd.concat([df_oil, df_oil_test1], axis=1)\n",
    "oil_2017.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_train = pd.read_csv(\n",
    "    'train.csv', usecols=[1, 2, 3, 4, 5],\n",
    "    dtype={'onpromotion': bool},\n",
    "    converters={'unit_sales': lambda u: np.log1p(\n",
    "        float(u)) if float(u) > 0 else 0},\n",
    "    parse_dates=[\"date\"],\n",
    "    skiprows=range(1, 66458909)  # 2016-01-01\n",
    ")\n",
    "df_2017 = df_train.loc[df_train.date>=pd.datetime(2017,1,1)]\n",
    "#del df_train\n",
    "df_test = pd.read_csv(\n",
    "    \"test.csv\", usecols=[0, 1, 2, 3, 4],\n",
    "    dtype={'onpromotion': bool},\n",
    "    parse_dates=[\"date\"]  # , date_parser=parser\n",
    ").set_index(\n",
    "    ['store_nbr', 'item_nbr', 'date']\n",
    ")\n",
    "promo_2017_train = df_2017.set_index(\n",
    "    [\"store_nbr\", \"item_nbr\", \"date\"])[[\"onpromotion\"]].unstack(\n",
    "        level=-1).fillna(False)\n",
    "promo_2017_train.columns = promo_2017_train.columns.get_level_values(1)\n",
    "promo_2017_test = df_test[[\"onpromotion\"]].unstack(level=-1).fillna(False)\n",
    "promo_2017_test.columns = promo_2017_test.columns.get_level_values(1)\n",
    "promo_2017_test = promo_2017_test.reindex(promo_2017_train.index).fillna(False)\n",
    "\n",
    "promo_2017 = pd.concat([promo_2017_train, promo_2017_test], axis=1)\n",
    "#del promo_2017_test, promo_2017_train\n",
    "\n",
    "df_2017 = df_2017.set_index(\n",
    "    [\"store_nbr\", \"item_nbr\", \"date\"])[[\"unit_sales\"]].unstack(\n",
    "        level=-1).fillna(0)\n",
    "\n",
    "df_2017.columns = df_2017.columns.get_level_values(1)\n",
    "\n",
    "items = items.reindex(df_2017.index.get_level_values(1))\n",
    "\n",
    "def get_timespan(df, dt, minus, periods, freq='D'):\n",
    "    return df[pd.date_range(dt - timedelta(days=minus), periods=periods, freq=freq)]\n",
    "\n",
    "def prepare_dataset(t2017, is_train=True):\n",
    "    X = pd.DataFrame({\n",
    "        \"day_1_2017\": get_timespan(df_2017, t2017, 1, 1).values.ravel(),\n",
    "        \"mean_3_2017\": get_timespan(df_2017, t2017, 3, 3).mean(axis=1).values,\n",
    "        \"mean_7_2017\": get_timespan(df_2017, t2017, 7, 7).mean(axis=1).values,\n",
    "        \"mean_14_2017\": get_timespan(df_2017, t2017, 14, 14).mean(axis=1).values,\n",
    "        \"mean_30_2017\": get_timespan(df_2017, t2017, 30, 30).mean(axis=1).values,\n",
    "        \"mean_60_2017\": get_timespan(df_2017, t2017, 60, 60).mean(axis=1).values,\n",
    "        \"mean_140_2017\": get_timespan(df_2017, t2017, 140, 140).mean(axis=1).values,\n",
    "        \"promo_7_2017\": get_timespan(promo_2017, t2017, 7, 7).sum(axis=1).values,\n",
    "        \"promo_14_2017\": get_timespan(promo_2017, t2017, 14, 14).sum(axis=1).values,\n",
    "        \"promo_30_2017\": get_timespan(promo_2017, t2017, 30, 30).sum(axis=1).values,\n",
    "        \"promo_60_2017\": get_timespan(promo_2017, t2017, 60, 60).sum(axis=1).values,\n",
    "        \"promo_140_2017\": get_timespan(promo_2017, t2017, 140, 140).sum(axis=1).values,\n",
    "        \"oil_7_2017\": get_timespan(oil_2017, t2017, 7, 7).mean(axis=1).values,\n",
    "        \"oil_14_2017\": get_timespan(oil_2017, t2017, 14, 14).mean(axis=1).values,\n",
    "        \"oil_30_2017\": get_timespan(oil_2017, t2017, 30, 30).mean(axis=1).values,\n",
    "        \"oil_60_2017\": get_timespan(oil_2017, t2017, 60, 60).mean(axis=1).values,\n",
    "        \"oil_140_2017\": get_timespan(oil_2017, t2017, 140, 140).mean(axis=1).values,\n",
    "        \n",
    "        \n",
    "    })\n",
    "    for i in range(7):\n",
    "        X['mean_4_dow{}_2017'.format(i)] = get_timespan(df_2017, t2017, 28-i, 4, freq='7D').mean(axis=1).values\n",
    "        X['mean_20_dow{}_2017'.format(i)] = get_timespan(df_2017, t2017, 140-i, 20, freq='7D').mean(axis=1).values\n",
    "    for i in range(16):\n",
    "        X[\"promo_{}\".format(i)] = promo_2017[\n",
    "            t2017 + timedelta(days=i)].values.astype(np.uint8)\n",
    "        X[\"oil_{}\".format(i)] = oil_2017[\n",
    "            t2017 + timedelta(days=i)].values.astype(np.uint8)\n",
    "        \n",
    "    if is_train:\n",
    "        y = df_2017[\n",
    "            pd.date_range(t2017, periods=16)\n",
    "        ].values\n",
    "        return X, y\n",
    "    return X\n",
    "\n",
    "print(\"Preparing dataset...\")\n",
    "t2017 = date(2017, 5, 31)\n",
    "X_l, y_l = [], []\n",
    "for i in range(6):\n",
    "    delta = timedelta(days=7 * i)\n",
    "    X_tmp, y_tmp = prepare_dataset(\n",
    "        t2017 + delta\n",
    "    )\n",
    "    X_l.append(X_tmp)\n",
    "    y_l.append(y_tmp)\n",
    "X_train = pd.concat(X_l, axis=0)\n",
    "y_train = np.concatenate(y_l, axis=0)\n",
    "del X_l, y_l\n",
    "X_val, y_val = prepare_dataset(date(2017, 7, 26))\n",
    "X_test = prepare_dataset(date(2017, 8, 16), is_train=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting models...\n",
      "==================================================\n",
      "Step 1\n",
      "==================================================\n",
      "[0]\ttrain-rmse:1.10315\tval-rmse:1.06062\n",
      "Multiple eval metrics have been passed: 'val-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until val-rmse hasn't improved in 50 rounds.\n",
      "[50]\ttrain-rmse:0.55387\tval-rmse:0.546575\n",
      "[100]\ttrain-rmse:0.549348\tval-rmse:0.544162\n",
      "[150]\ttrain-rmse:0.547188\tval-rmse:0.543021\n",
      "[199]\ttrain-rmse:0.545688\tval-rmse:0.542656\n",
      "==================================================\n",
      "Step 2\n",
      "==================================================\n",
      "[0]\ttrain-rmse:1.03352\tval-rmse:1.00521\n",
      "Multiple eval metrics have been passed: 'val-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until val-rmse hasn't improved in 50 rounds.\n",
      "[50]\ttrain-rmse:0.571829\tval-rmse:0.574897\n",
      "[100]\ttrain-rmse:0.568249\tval-rmse:0.572619\n",
      "[150]\ttrain-rmse:0.566325\tval-rmse:0.571444\n",
      "[199]\ttrain-rmse:0.564943\tval-rmse:0.571106\n",
      "==================================================\n",
      "Step 3\n",
      "==================================================\n",
      "[0]\ttrain-rmse:1.11701\tval-rmse:1.10885\n",
      "Multiple eval metrics have been passed: 'val-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until val-rmse hasn't improved in 50 rounds.\n",
      "[50]\ttrain-rmse:0.577736\tval-rmse:0.589054\n",
      "[100]\ttrain-rmse:0.572741\tval-rmse:0.586759\n",
      "[150]\ttrain-rmse:0.570212\tval-rmse:0.585835\n",
      "[199]\ttrain-rmse:0.568462\tval-rmse:0.58546\n",
      "==================================================\n",
      "Step 4\n",
      "==================================================\n",
      "[0]\ttrain-rmse:1.25219\tval-rmse:1.20888\n",
      "Multiple eval metrics have been passed: 'val-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until val-rmse hasn't improved in 50 rounds.\n",
      "[50]\ttrain-rmse:0.595178\tval-rmse:0.599535\n",
      "[100]\ttrain-rmse:0.590283\tval-rmse:0.596595\n",
      "[150]\ttrain-rmse:0.587717\tval-rmse:0.595378\n",
      "[199]\ttrain-rmse:0.585853\tval-rmse:0.594988\n",
      "==================================================\n",
      "Step 5\n",
      "==================================================\n",
      "[0]\ttrain-rmse:1.29125\tval-rmse:1.24564\n",
      "Multiple eval metrics have been passed: 'val-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until val-rmse hasn't improved in 50 rounds.\n",
      "[50]\ttrain-rmse:0.60312\tval-rmse:0.603807\n",
      "[100]\ttrain-rmse:0.597765\tval-rmse:0.600527\n",
      "[150]\ttrain-rmse:0.594722\tval-rmse:0.598963\n",
      "[199]\ttrain-rmse:0.592576\tval-rmse:0.598113\n",
      "==================================================\n",
      "Step 6\n",
      "==================================================\n",
      "[0]\ttrain-rmse:1.12152\tval-rmse:1.15049\n",
      "Multiple eval metrics have been passed: 'val-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until val-rmse hasn't improved in 50 rounds.\n",
      "[50]\ttrain-rmse:0.599225\tval-rmse:0.609266\n",
      "[100]\ttrain-rmse:0.595126\tval-rmse:0.607377\n",
      "[150]\ttrain-rmse:0.592608\tval-rmse:0.606066\n",
      "[199]\ttrain-rmse:0.590835\tval-rmse:0.605515\n",
      "==================================================\n",
      "Step 7\n",
      "==================================================\n",
      "[0]\ttrain-rmse:1.08908\tval-rmse:1.21475\n",
      "Multiple eval metrics have been passed: 'val-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until val-rmse hasn't improved in 50 rounds.\n",
      "[50]\ttrain-rmse:0.592043\tval-rmse:0.659698\n",
      "[100]\ttrain-rmse:0.587814\tval-rmse:0.657942\n",
      "[150]\ttrain-rmse:0.585346\tval-rmse:0.657623\n",
      "[199]\ttrain-rmse:0.583606\tval-rmse:0.657495\n",
      "==================================================\n",
      "Step 8\n",
      "==================================================\n",
      "[0]\ttrain-rmse:1.0988\tval-rmse:1.194\n",
      "Multiple eval metrics have been passed: 'val-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until val-rmse hasn't improved in 50 rounds.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-a210e7e1bd65>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mwatchlist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'val'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_rounds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwatchlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0mval_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jillm\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[0;32m    202\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jillm\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jillm\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m    896\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[1;32m--> 898\u001b[1;33m                                                     dtrain.handle))\n\u001b[0m\u001b[0;32m    899\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    900\u001b[0m             \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Training and predicting models...\")\n",
    "\n",
    "param = {}\n",
    "param['objective'] = 'reg:linear'\n",
    "param['eta'] = 0.1\n",
    "param['max_depth'] = 5\n",
    "param['silent'] = 0\n",
    "param['eval_metric'] = 'rmse'\n",
    "param['min_child_weight'] = 5\n",
    "param['subsample'] = 0.5\n",
    "param['colsample_bytree'] = 0.7\n",
    "param['seed'] = 42\n",
    "num_rounds = 200\n",
    "\n",
    "\n",
    "\n",
    "plst = list(param.items())\n",
    "\n",
    "MAX_ROUNDS = 200\n",
    "val_pred = []\n",
    "test_pred = []\n",
    "cate_vars = []\n",
    "\n",
    "dtest = xgb.DMatrix(X_test)\n",
    "for i in range(16):\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Step %d\" % (i+1))\n",
    "    print(\"=\" * 50)\n",
    "    dtrain = xgb.DMatrix(\n",
    "        X_train, label=y_train[:, i],\n",
    "        weight=pd.concat([items[\"perishable\"]] * 6) * 0.25 + 1\n",
    "    )\n",
    "    dval = xgb.DMatrix(\n",
    "        X_val, label=y_val[:, i],\n",
    "        weight=items[\"perishable\"] * 0.25 + 1)\n",
    "        \n",
    "    watchlist = [ (dtrain,'train'), (dval, 'val') ]\n",
    "    model = xgb.train(plst, dtrain, num_rounds, watchlist, early_stopping_rounds=50, verbose_eval=50)\n",
    "    \n",
    "    val_pred.append(model.predict(dval))\n",
    "    test_pred.append(model.predict(dtest))\n",
    "\n",
    "print(\"Validation mse:\", mean_squared_error(\n",
    "    y_val, np.array(val_pred).transpose()))\n",
    "\n",
    "print(\"Making submission...\")\n",
    "y_test = np.array(test_pred).transpose()\n",
    "df_preds = pd.DataFrame(\n",
    "    y_test, index=df_2017.index,\n",
    "    columns=pd.date_range(\"2017-08-16\", periods=16)\n",
    ").stack().to_frame(\"unit_sales\")\n",
    "df_preds.index.set_names([\"store_nbr\", \"item_nbr\", \"date\"], inplace=True)\n",
    "\n",
    "submission = df_test[[\"id\"]].join(df_preds, how=\"left\").fillna(0)\n",
    "submission[\"unit_sales\"] = np.clip(np.expm1(submission[\"unit_sales\"]), 0, 1000)\n",
    "submission.to_csv('xgb2.csv', float_format='%.4f', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting models...\n",
      "==================================================\n",
      "Step 1\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jillm\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\lightgbm\\engine.py:99: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\jillm\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\lightgbm\\basic.py:1027: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds.\n",
      "[50]\ttraining's l2: 0.620696\tvalid_1's l2: 0.591616\n",
      "[100]\ttraining's l2: 0.429369\tvalid_1's l2: 0.410232\n",
      "[150]\ttraining's l2: 0.355506\tvalid_1's l2: 0.341577\n",
      "[200]\ttraining's l2: 0.32606\tvalid_1's l2: 0.315151\n",
      "[250]\ttraining's l2: 0.313431\tvalid_1's l2: 0.304298\n",
      "[300]\ttraining's l2: 0.307486\tvalid_1's l2: 0.299571\n",
      "[350]\ttraining's l2: 0.304452\tvalid_1's l2: 0.297425\n",
      "[400]\ttraining's l2: 0.302537\tvalid_1's l2: 0.296176\n",
      "[450]\ttraining's l2: 0.301166\tvalid_1's l2: 0.295377\n",
      "[500]\ttraining's l2: 0.300138\tvalid_1's l2: 0.294746\n",
      "[550]\ttraining's l2: 0.299351\tvalid_1's l2: 0.294326\n",
      "[600]\ttraining's l2: 0.298683\tvalid_1's l2: 0.293985\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[600]\ttraining's l2: 0.298683\tvalid_1's l2: 0.293985\n",
      "mean_7_2017: 15398178.63\n",
      "mean_14_2017: 11134647.66\n",
      "mean_30_2017: 2004574.87\n",
      "mean_3_2017: 1069893.05\n",
      "promo_0: 925842.35\n",
      "mean_20_dow0_2017: 679035.42\n",
      "day_1_2017: 412854.45\n",
      "mean_4_dow0_2017: 406925.29\n",
      "mean_60_2017: 264810.36\n",
      "promo_7_2017: 234201.62\n",
      "mean_140_2017: 68434.90\n",
      "promo_14_2017: 68322.04\n",
      "promo_7: 64461.64\n",
      "promo_140_2017: 45243.11\n",
      "oil_140_2017: 45231.56\n",
      "mean_4_dow5_2017: 44670.14\n",
      "promo_30_2017: 42614.46\n",
      "mean_20_dow4_2017: 38248.74\n",
      "mean_20_dow2_2017: 35135.46\n",
      "promo_60_2017: 26907.00\n",
      "promo_14: 24649.49\n",
      "mean_4_dow2_2017: 19952.25\n",
      "mean_4_dow6_2017: 19874.62\n",
      "promo_9: 19044.93\n",
      "mean_20_dow1_2017: 18290.68\n",
      "mean_20_dow3_2017: 11142.83\n",
      "promo_15: 10772.03\n",
      "oil_14_2017: 10737.03\n",
      "mean_4_dow4_2017: 8661.32\n",
      "promo_11: 7962.89\n",
      "promo_2: 7350.46\n",
      "mean_4_dow1_2017: 7193.68\n",
      "promo_13: 6878.44\n",
      "mean_20_dow6_2017: 6254.22\n",
      "mean_4_dow3_2017: 5448.27\n",
      "mean_20_dow5_2017: 5103.28\n",
      "promo_1: 5028.17\n",
      "promo_3: 4462.75\n",
      "promo_12: 4407.74\n",
      "promo_4: 3318.75\n",
      "oil_60_2017: 3142.59\n",
      "oil_30_2017: 2973.72\n",
      "promo_10: 2876.87\n",
      "oil_7_2017: 2517.04\n",
      "promo_5: 1363.53\n",
      "promo_8: 1152.42\n",
      "promo_6: 766.80\n",
      "oil_0: 0.00\n",
      "oil_1: 0.00\n",
      "oil_2: 0.00\n",
      "oil_3: 0.00\n",
      "oil_4: 0.00\n",
      "oil_5: 0.00\n",
      "oil_6: 0.00\n",
      "oil_7: 0.00\n",
      "oil_8: 0.00\n",
      "oil_9: 0.00\n",
      "oil_10: 0.00\n",
      "oil_11: 0.00\n",
      "oil_12: 0.00\n",
      "oil_13: 0.00\n",
      "oil_14: 0.00\n",
      "oil_15: 0.00\n",
      "==================================================\n",
      "Step 2\n",
      "==================================================\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[50]\ttraining's l2: 0.589507\tvalid_1's l2: 0.582337\n",
      "[100]\ttraining's l2: 0.429166\tvalid_1's l2: 0.428549\n",
      "[150]\ttraining's l2: 0.367955\tvalid_1's l2: 0.369919\n",
      "[200]\ttraining's l2: 0.343518\tvalid_1's l2: 0.346403\n",
      "[250]\ttraining's l2: 0.333182\tvalid_1's l2: 0.336487\n",
      "[300]\ttraining's l2: 0.32837\tvalid_1's l2: 0.331947\n",
      "[350]\ttraining's l2: 0.325821\tvalid_1's l2: 0.329694\n",
      "[400]\ttraining's l2: 0.324285\tvalid_1's l2: 0.328476\n",
      "[450]\ttraining's l2: 0.323173\tvalid_1's l2: 0.327607\n",
      "[500]\ttraining's l2: 0.322349\tvalid_1's l2: 0.326976\n",
      "[550]\ttraining's l2: 0.321623\tvalid_1's l2: 0.326428\n",
      "[600]\ttraining's l2: 0.321064\tvalid_1's l2: 0.326073\n",
      "[650]\ttraining's l2: 0.320504\tvalid_1's l2: 0.325595\n",
      "[700]\ttraining's l2: 0.320084\tvalid_1's l2: 0.325321\n",
      "[750]\ttraining's l2: 0.319736\tvalid_1's l2: 0.325096\n",
      "[800]\ttraining's l2: 0.319439\tvalid_1's l2: 0.32494\n",
      "[850]\ttraining's l2: 0.319125\tvalid_1's l2: 0.324725\n",
      "[900]\ttraining's l2: 0.318817\tvalid_1's l2: 0.324542\n",
      "[950]\ttraining's l2: 0.318556\tvalid_1's l2: 0.32441\n",
      "[1000]\ttraining's l2: 0.318309\tvalid_1's l2: 0.324289\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's l2: 0.318309\tvalid_1's l2: 0.324289\n",
      "mean_14_2017: 13289864.77\n",
      "mean_7_2017: 8454081.56\n",
      "mean_30_2017: 2368411.97\n",
      "mean_60_2017: 757799.19\n",
      "mean_20_dow1_2017: 680351.47\n",
      "promo_1: 554332.36\n",
      "mean_3_2017: 479812.47\n",
      "day_1_2017: 164816.38\n",
      "oil_140_2017: 129382.65\n",
      "mean_4_dow1_2017: 122717.98\n",
      "promo_7_2017: 107744.24\n",
      "promo_14_2017: 100475.96\n",
      "mean_140_2017: 70278.62\n",
      "promo_30_2017: 48186.00\n",
      "mean_4_dow2_2017: 45654.45\n",
      "promo_3: 42541.61\n",
      "mean_20_dow4_2017: 39170.84\n",
      "mean_20_dow2_2017: 38498.54\n",
      "oil_14_2017: 32843.73\n",
      "promo_140_2017: 29689.58\n",
      "promo_60_2017: 29066.60\n",
      "promo_0: 28814.93\n",
      "promo_4: 23788.58\n",
      "mean_4_dow6_2017: 18825.60\n",
      "promo_5: 17919.64\n",
      "promo_2: 17205.86\n",
      "mean_20_dow0_2017: 14989.94\n",
      "mean_4_dow4_2017: 14822.05\n",
      "oil_30_2017: 13494.56\n",
      "mean_20_dow6_2017: 13021.99\n",
      "mean_4_dow0_2017: 11886.39\n",
      "oil_7_2017: 10877.71\n",
      "mean_20_dow5_2017: 10442.51\n",
      "mean_4_dow3_2017: 8500.11\n",
      "promo_7: 8284.67\n",
      "promo_6: 7888.86\n",
      "oil_60_2017: 7797.23\n",
      "mean_4_dow5_2017: 6915.51\n",
      "mean_20_dow3_2017: 6258.08\n",
      "promo_14: 3559.19\n",
      "promo_9: 2406.60\n",
      "promo_8: 2323.57\n",
      "promo_13: 1994.14\n",
      "promo_11: 1562.95\n",
      "promo_10: 1157.03\n",
      "promo_15: 1104.65\n",
      "promo_12: 599.71\n",
      "oil_0: 0.00\n",
      "oil_1: 0.00\n",
      "oil_2: 0.00\n",
      "oil_3: 0.00\n",
      "oil_4: 0.00\n",
      "oil_5: 0.00\n",
      "oil_6: 0.00\n",
      "oil_7: 0.00\n",
      "oil_8: 0.00\n",
      "oil_9: 0.00\n",
      "oil_10: 0.00\n",
      "oil_11: 0.00\n",
      "oil_12: 0.00\n",
      "oil_13: 0.00\n",
      "oil_14: 0.00\n",
      "oil_15: 0.00\n",
      "==================================================\n",
      "Step 3\n",
      "==================================================\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[50]\ttraining's l2: 0.643434\tvalid_1's l2: 0.648385\n",
      "[100]\ttraining's l2: 0.456196\tvalid_1's l2: 0.465549\n",
      "[150]\ttraining's l2: 0.383251\tvalid_1's l2: 0.39479\n",
      "[200]\ttraining's l2: 0.353882\tvalid_1's l2: 0.366386\n",
      "[250]\ttraining's l2: 0.341372\tvalid_1's l2: 0.354376\n",
      "[300]\ttraining's l2: 0.335331\tvalid_1's l2: 0.348794\n",
      "[350]\ttraining's l2: 0.331988\tvalid_1's l2: 0.345904\n",
      "[400]\ttraining's l2: 0.329919\tvalid_1's l2: 0.344239\n",
      "[450]\ttraining's l2: 0.328411\tvalid_1's l2: 0.343173\n",
      "[500]\ttraining's l2: 0.327219\tvalid_1's l2: 0.342345\n",
      "[550]\ttraining's l2: 0.326182\tvalid_1's l2: 0.341615\n",
      "[600]\ttraining's l2: 0.325355\tvalid_1's l2: 0.341155\n",
      "[650]\ttraining's l2: 0.324674\tvalid_1's l2: 0.340776\n",
      "[700]\ttraining's l2: 0.324128\tvalid_1's l2: 0.34059\n",
      "[750]\ttraining's l2: 0.323606\tvalid_1's l2: 0.34036\n",
      "[800]\ttraining's l2: 0.323139\tvalid_1's l2: 0.340195\n",
      "[850]\ttraining's l2: 0.322759\tvalid_1's l2: 0.34002\n",
      "[900]\ttraining's l2: 0.322372\tvalid_1's l2: 0.339896\n",
      "[950]\ttraining's l2: 0.322005\tvalid_1's l2: 0.339807\n",
      "[1000]\ttraining's l2: 0.321654\tvalid_1's l2: 0.339715\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's l2: 0.321654\tvalid_1's l2: 0.339715\n",
      "mean_14_2017: 16194184.99\n",
      "mean_7_2017: 7069571.55\n",
      "mean_30_2017: 2672362.00\n",
      "mean_20_dow2_2017: 2002405.54\n",
      "mean_4_dow2_2017: 1881960.49\n",
      "promo_2: 841153.72\n",
      "mean_60_2017: 539819.98\n",
      "mean_3_2017: 241183.59\n",
      "promo_14_2017: 131928.75\n",
      "promo_3: 90610.95\n",
      "promo_7_2017: 83335.50\n",
      "promo_140_2017: 82255.26\n",
      "day_1_2017: 80192.03\n",
      "promo_30_2017: 78732.52\n",
      "oil_140_2017: 76037.20\n",
      "mean_20_dow4_2017: 59494.55\n",
      "promo_9: 57251.01\n",
      "promo_60_2017: 50690.29\n",
      "mean_20_dow1_2017: 39002.93\n",
      "mean_4_dow3_2017: 35612.14\n",
      "mean_4_dow1_2017: 32679.57\n",
      "promo_5: 31399.43\n",
      "mean_20_dow5_2017: 29814.57\n",
      "promo_4: 29113.23\n",
      "promo_7: 26241.13\n",
      "oil_14_2017: 25641.27\n",
      "mean_140_2017: 25587.86\n",
      "mean_4_dow0_2017: 21094.11\n",
      "promo_0: 20273.88\n",
      "mean_20_dow0_2017: 19467.90\n",
      "oil_30_2017: 16364.98\n",
      "promo_1: 14607.51\n",
      "mean_4_dow4_2017: 14525.78\n",
      "promo_14: 12217.50\n",
      "promo_11: 12071.95\n",
      "promo_15: 11212.05\n",
      "mean_20_dow3_2017: 10768.69\n",
      "mean_4_dow6_2017: 10616.21\n",
      "mean_20_dow6_2017: 10270.14\n",
      "promo_6: 9958.10\n",
      "promo_10: 7532.92\n",
      "mean_4_dow5_2017: 6272.74\n",
      "promo_13: 6099.52\n",
      "promo_8: 5466.30\n",
      "oil_60_2017: 5296.37\n",
      "promo_12: 4941.66\n",
      "oil_7_2017: 4053.67\n",
      "oil_0: 0.00\n",
      "oil_1: 0.00\n",
      "oil_2: 0.00\n",
      "oil_3: 0.00\n",
      "oil_4: 0.00\n",
      "oil_5: 0.00\n",
      "oil_6: 0.00\n",
      "oil_7: 0.00\n",
      "oil_8: 0.00\n",
      "oil_9: 0.00\n",
      "oil_10: 0.00\n",
      "oil_11: 0.00\n",
      "oil_12: 0.00\n",
      "oil_13: 0.00\n",
      "oil_14: 0.00\n",
      "oil_15: 0.00\n",
      "==================================================\n",
      "Step 4\n",
      "==================================================\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[50]\ttraining's l2: 0.709031\tvalid_1's l2: 0.687997\n",
      "[100]\ttraining's l2: 0.492434\tvalid_1's l2: 0.48354\n",
      "[150]\ttraining's l2: 0.409362\tvalid_1's l2: 0.407589\n",
      "[200]\ttraining's l2: 0.376368\tvalid_1's l2: 0.378385\n",
      "[250]\ttraining's l2: 0.362418\tvalid_1's l2: 0.366353\n",
      "[300]\ttraining's l2: 0.355962\tvalid_1's l2: 0.361126\n",
      "[350]\ttraining's l2: 0.352525\tvalid_1's l2: 0.358415\n",
      "[400]\ttraining's l2: 0.350475\tvalid_1's l2: 0.356906\n",
      "[450]\ttraining's l2: 0.348963\tvalid_1's l2: 0.355916\n",
      "[500]\ttraining's l2: 0.347832\tvalid_1's l2: 0.355162\n",
      "[550]\ttraining's l2: 0.346877\tvalid_1's l2: 0.354575\n",
      "[600]\ttraining's l2: 0.345831\tvalid_1's l2: 0.353915\n",
      "[650]\ttraining's l2: 0.345056\tvalid_1's l2: 0.353513\n",
      "[700]\ttraining's l2: 0.344337\tvalid_1's l2: 0.353144\n",
      "[750]\ttraining's l2: 0.34378\tvalid_1's l2: 0.352869\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[800]\ttraining's l2: 0.343243\tvalid_1's l2: 0.352553\n",
      "[850]\ttraining's l2: 0.34275\tvalid_1's l2: 0.352314\n",
      "[900]\ttraining's l2: 0.342313\tvalid_1's l2: 0.352116\n",
      "[950]\ttraining's l2: 0.341893\tvalid_1's l2: 0.351939\n",
      "[1000]\ttraining's l2: 0.341528\tvalid_1's l2: 0.351786\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's l2: 0.341528\tvalid_1's l2: 0.351786\n",
      "mean_14_2017: 20028839.58\n",
      "mean_7_2017: 5474677.18\n",
      "mean_30_2017: 5178644.61\n",
      "mean_20_dow3_2017: 1587858.43\n",
      "mean_4_dow3_2017: 1491500.39\n",
      "mean_60_2017: 1237022.38\n",
      "promo_3: 634330.44\n",
      "mean_3_2017: 443670.07\n",
      "mean_4_dow4_2017: 434256.70\n",
      "oil_14_2017: 111009.76\n",
      "promo_14_2017: 96694.94\n",
      "promo_7_2017: 88320.52\n",
      "mean_140_2017: 81028.16\n",
      "oil_140_2017: 79047.22\n",
      "promo_30_2017: 59652.84\n",
      "promo_5: 55003.30\n",
      "day_1_2017: 54050.76\n",
      "promo_140_2017: 53221.66\n",
      "promo_4: 44623.60\n",
      "mean_4_dow2_2017: 40270.43\n",
      "oil_30_2017: 40090.92\n",
      "promo_60_2017: 37397.48\n",
      "promo_7: 33766.37\n",
      "mean_20_dow4_2017: 32962.93\n",
      "promo_2: 32959.61\n",
      "mean_20_dow5_2017: 29829.03\n",
      "oil_7_2017: 29808.05\n",
      "mean_20_dow0_2017: 27604.39\n",
      "mean_20_dow6_2017: 19821.71\n",
      "promo_6: 19204.70\n",
      "mean_4_dow0_2017: 16955.62\n",
      "mean_20_dow2_2017: 16749.02\n",
      "promo_0: 14778.18\n",
      "promo_1: 14483.48\n",
      "promo_14: 10522.98\n",
      "mean_20_dow1_2017: 10108.60\n",
      "mean_4_dow1_2017: 9527.07\n",
      "mean_4_dow6_2017: 9288.08\n",
      "mean_4_dow5_2017: 8663.01\n",
      "promo_9: 7772.22\n",
      "promo_10: 6194.18\n",
      "promo_8: 5284.12\n",
      "oil_60_2017: 4076.59\n",
      "promo_15: 3612.86\n",
      "promo_13: 2854.46\n",
      "promo_11: 1637.68\n",
      "promo_12: 1284.45\n",
      "oil_0: 0.00\n",
      "oil_1: 0.00\n",
      "oil_2: 0.00\n",
      "oil_3: 0.00\n",
      "oil_4: 0.00\n",
      "oil_5: 0.00\n",
      "oil_6: 0.00\n",
      "oil_7: 0.00\n",
      "oil_8: 0.00\n",
      "oil_9: 0.00\n",
      "oil_10: 0.00\n",
      "oil_11: 0.00\n",
      "oil_12: 0.00\n",
      "oil_13: 0.00\n",
      "oil_14: 0.00\n",
      "oil_15: 0.00\n",
      "==================================================\n",
      "Step 5\n",
      "==================================================\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[50]\ttraining's l2: 0.745737\tvalid_1's l2: 0.719568\n",
      "[100]\ttraining's l2: 0.514706\tvalid_1's l2: 0.502105\n",
      "[150]\ttraining's l2: 0.424713\tvalid_1's l2: 0.419611\n",
      "[200]\ttraining's l2: 0.388698\tvalid_1's l2: 0.387135\n",
      "[250]\ttraining's l2: 0.373316\tvalid_1's l2: 0.373501\n",
      "[300]\ttraining's l2: 0.366075\tvalid_1's l2: 0.367075\n",
      "[350]\ttraining's l2: 0.362134\tvalid_1's l2: 0.363746\n",
      "[400]\ttraining's l2: 0.359666\tvalid_1's l2: 0.361694\n",
      "[450]\ttraining's l2: 0.357904\tvalid_1's l2: 0.360365\n",
      "[500]\ttraining's l2: 0.356528\tvalid_1's l2: 0.359436\n",
      "[550]\ttraining's l2: 0.355398\tvalid_1's l2: 0.358789\n",
      "[600]\ttraining's l2: 0.354282\tvalid_1's l2: 0.358088\n",
      "[650]\ttraining's l2: 0.353344\tvalid_1's l2: 0.35756\n",
      "[700]\ttraining's l2: 0.352518\tvalid_1's l2: 0.357114\n",
      "[750]\ttraining's l2: 0.351843\tvalid_1's l2: 0.356767\n",
      "[800]\ttraining's l2: 0.351275\tvalid_1's l2: 0.356422\n",
      "[850]\ttraining's l2: 0.350707\tvalid_1's l2: 0.356099\n",
      "[900]\ttraining's l2: 0.350208\tvalid_1's l2: 0.355797\n",
      "[950]\ttraining's l2: 0.349751\tvalid_1's l2: 0.35559\n",
      "[1000]\ttraining's l2: 0.349312\tvalid_1's l2: 0.355379\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's l2: 0.349312\tvalid_1's l2: 0.355379\n",
      "mean_14_2017: 13727168.38\n",
      "mean_4_dow4_2017: 13534820.58\n",
      "mean_7_2017: 3792852.16\n",
      "mean_20_dow4_2017: 2673573.25\n",
      "mean_30_2017: 2372009.65\n",
      "mean_3_2017: 1490902.66\n",
      "promo_4: 619712.20\n",
      "mean_60_2017: 556437.45\n",
      "oil_140_2017: 168164.81\n",
      "mean_4_dow3_2017: 135401.40\n",
      "oil_14_2017: 129948.39\n",
      "promo_7_2017: 114915.35\n",
      "promo_5: 84932.98\n",
      "promo_14_2017: 68948.61\n",
      "promo_3: 67701.37\n",
      "promo_30_2017: 59437.92\n",
      "mean_140_2017: 57991.27\n",
      "promo_7: 57433.89\n",
      "oil_7_2017: 47686.88\n",
      "promo_140_2017: 46506.21\n",
      "promo_60_2017: 42589.91\n",
      "day_1_2017: 42183.10\n",
      "mean_20_dow1_2017: 37192.96\n",
      "promo_2: 36466.04\n",
      "mean_20_dow0_2017: 32798.98\n",
      "mean_20_dow2_2017: 31195.08\n",
      "mean_20_dow3_2017: 30844.13\n",
      "promo_6: 28348.57\n",
      "promo_11: 18449.51\n",
      "promo_1: 16975.24\n",
      "mean_4_dow0_2017: 15228.80\n",
      "promo_14: 14915.64\n",
      "mean_20_dow5_2017: 14047.31\n",
      "promo_0: 13665.18\n",
      "mean_20_dow6_2017: 12278.30\n",
      "mean_4_dow5_2017: 12191.73\n",
      "mean_4_dow6_2017: 10583.53\n",
      "mean_4_dow2_2017: 10244.19\n",
      "promo_9: 9828.54\n",
      "mean_4_dow1_2017: 8294.20\n",
      "promo_10: 7909.23\n",
      "oil_60_2017: 7706.97\n",
      "promo_13: 7079.18\n",
      "promo_8: 7067.62\n",
      "oil_30_2017: 5537.44\n",
      "promo_15: 5537.21\n",
      "promo_12: 4084.65\n",
      "oil_0: 0.00\n",
      "oil_1: 0.00\n",
      "oil_2: 0.00\n",
      "oil_3: 0.00\n",
      "oil_4: 0.00\n",
      "oil_5: 0.00\n",
      "oil_6: 0.00\n",
      "oil_7: 0.00\n",
      "oil_8: 0.00\n",
      "oil_9: 0.00\n",
      "oil_10: 0.00\n",
      "oil_11: 0.00\n",
      "oil_12: 0.00\n",
      "oil_13: 0.00\n",
      "oil_14: 0.00\n",
      "oil_15: 0.00\n",
      "==================================================\n",
      "Step 6\n",
      "==================================================\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[50]\ttraining's l2: 0.656294\tvalid_1's l2: 0.681301\n",
      "[100]\ttraining's l2: 0.474811\tvalid_1's l2: 0.494363\n",
      "[150]\ttraining's l2: 0.405029\tvalid_1's l2: 0.421446\n",
      "[200]\ttraining's l2: 0.377451\tvalid_1's l2: 0.392171\n",
      "[250]\ttraining's l2: 0.365812\tvalid_1's l2: 0.379534\n",
      "[300]\ttraining's l2: 0.360429\tvalid_1's l2: 0.373543\n",
      "[350]\ttraining's l2: 0.357507\tvalid_1's l2: 0.370415\n",
      "[400]\ttraining's l2: 0.355721\tvalid_1's l2: 0.368723\n",
      "[450]\ttraining's l2: 0.354424\tvalid_1's l2: 0.367798\n",
      "[500]\ttraining's l2: 0.353375\tvalid_1's l2: 0.367057\n",
      "[550]\ttraining's l2: 0.352381\tvalid_1's l2: 0.366417\n",
      "[600]\ttraining's l2: 0.351449\tvalid_1's l2: 0.365921\n",
      "[650]\ttraining's l2: 0.350589\tvalid_1's l2: 0.365325\n",
      "[700]\ttraining's l2: 0.349977\tvalid_1's l2: 0.365078\n",
      "[750]\ttraining's l2: 0.349408\tvalid_1's l2: 0.364761\n",
      "[800]\ttraining's l2: 0.348945\tvalid_1's l2: 0.364553\n",
      "[850]\ttraining's l2: 0.348528\tvalid_1's l2: 0.36439\n",
      "[900]\ttraining's l2: 0.348116\tvalid_1's l2: 0.364241\n",
      "[950]\ttraining's l2: 0.347763\tvalid_1's l2: 0.364113\n",
      "[1000]\ttraining's l2: 0.347451\tvalid_1's l2: 0.364007\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's l2: 0.347451\tvalid_1's l2: 0.364007\n",
      "mean_14_2017: 13378918.95\n",
      "mean_30_2017: 8676007.95\n",
      "mean_7_2017: 2852388.22\n",
      "mean_60_2017: 1822154.73\n",
      "mean_3_2017: 1375817.29\n",
      "mean_4_dow5_2017: 811833.33\n",
      "mean_20_dow5_2017: 774410.16\n",
      "promo_5: 702267.63\n",
      "promo_14_2017: 92510.57\n",
      "promo_3: 91146.87\n",
      "oil_14_2017: 84453.37\n",
      "promo_7_2017: 79752.28\n",
      "mean_140_2017: 70641.09\n",
      "promo_30_2017: 68908.38\n",
      "promo_7: 57650.43\n",
      "mean_20_dow6_2017: 56544.98\n",
      "promo_6: 53401.16\n",
      "day_1_2017: 46905.22\n",
      "mean_4_dow6_2017: 44413.63\n",
      "oil_140_2017: 43615.24\n",
      "promo_60_2017: 38278.12\n",
      "promo_140_2017: 34598.47\n",
      "mean_20_dow0_2017: 30029.86\n",
      "oil_7_2017: 27407.60\n",
      "oil_30_2017: 26702.83\n",
      "mean_20_dow3_2017: 25316.30\n",
      "promo_4: 24290.27\n",
      "mean_20_dow2_2017: 19758.68\n",
      "promo_2: 19681.41\n",
      "promo_9: 17167.78\n",
      "mean_4_dow0_2017: 16096.07\n",
      "mean_4_dow2_2017: 15884.94\n",
      "mean_4_dow4_2017: 12487.11\n",
      "mean_20_dow4_2017: 12017.68\n",
      "mean_4_dow1_2017: 11942.60\n",
      "promo_0: 11566.67\n",
      "mean_4_dow3_2017: 11277.03\n",
      "promo_14: 10398.34\n",
      "promo_1: 9806.47\n",
      "mean_20_dow1_2017: 9172.48\n",
      "promo_12: 8006.14\n",
      "promo_13: 7745.86\n",
      "promo_8: 7646.47\n",
      "promo_11: 7056.20\n",
      "promo_10: 6663.91\n",
      "promo_15: 3302.89\n",
      "oil_60_2017: 2570.53\n",
      "oil_0: 0.00\n",
      "oil_1: 0.00\n",
      "oil_2: 0.00\n",
      "oil_3: 0.00\n",
      "oil_4: 0.00\n",
      "oil_5: 0.00\n",
      "oil_6: 0.00\n",
      "oil_7: 0.00\n",
      "oil_8: 0.00\n",
      "oil_9: 0.00\n",
      "oil_10: 0.00\n",
      "oil_11: 0.00\n",
      "oil_12: 0.00\n",
      "oil_13: 0.00\n",
      "oil_14: 0.00\n",
      "oil_15: 0.00\n",
      "==================================================\n",
      "Step 7\n",
      "==================================================\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[50]\ttraining's l2: 0.636505\tvalid_1's l2: 0.767722\n",
      "[100]\ttraining's l2: 0.463398\tvalid_1's l2: 0.57303\n",
      "[150]\ttraining's l2: 0.396137\tvalid_1's l2: 0.493528\n",
      "[200]\ttraining's l2: 0.369027\tvalid_1's l2: 0.460055\n",
      "[250]\ttraining's l2: 0.35739\tvalid_1's l2: 0.445618\n",
      "[300]\ttraining's l2: 0.351898\tvalid_1's l2: 0.439174\n",
      "[350]\ttraining's l2: 0.34889\tvalid_1's l2: 0.435795\n",
      "[400]\ttraining's l2: 0.346987\tvalid_1's l2: 0.433728\n",
      "[450]\ttraining's l2: 0.345668\tvalid_1's l2: 0.433025\n",
      "[500]\ttraining's l2: 0.344607\tvalid_1's l2: 0.432493\n",
      "[550]\ttraining's l2: 0.343546\tvalid_1's l2: 0.432026\n",
      "[600]\ttraining's l2: 0.342598\tvalid_1's l2: 0.431898\n",
      "Early stopping, best iteration is:\n",
      "[590]\ttraining's l2: 0.342779\tvalid_1's l2: 0.431858\n",
      "mean_14_2017: 11891450.81\n",
      "mean_30_2017: 8159139.19\n",
      "mean_7_2017: 2850624.92\n",
      "mean_20_dow6_2017: 1419713.45\n",
      "mean_60_2017: 1400136.86\n",
      "mean_4_dow6_2017: 1165323.03\n",
      "promo_6: 1017327.62\n",
      "mean_3_2017: 793986.91\n",
      "day_1_2017: 100104.61\n",
      "promo_3: 96784.43\n",
      "promo_14_2017: 92376.06\n",
      "promo_7: 85886.91\n",
      "promo_5: 85459.96\n",
      "promo_30_2017: 78962.15\n",
      "mean_140_2017: 73738.38\n",
      "mean_20_dow5_2017: 66512.69\n",
      "mean_4_dow5_2017: 65919.58\n",
      "promo_7_2017: 64910.56\n",
      "oil_14_2017: 62936.21\n",
      "promo_13: 56738.99\n",
      "mean_20_dow1_2017: 42991.68\n",
      "promo_140_2017: 34408.13\n",
      "oil_140_2017: 33056.94\n",
      "promo_60_2017: 26324.47\n",
      "oil_30_2017: 23436.07\n",
      "promo_4: 21499.31\n",
      "mean_20_dow0_2017: 21498.60\n",
      "oil_7_2017: 18264.47\n",
      "promo_9: 16512.71\n",
      "promo_0: 14746.15\n",
      "mean_20_dow3_2017: 14121.92\n",
      "mean_4_dow1_2017: 13236.91\n",
      "promo_14: 12406.03\n",
      "promo_2: 12329.90\n",
      "mean_4_dow0_2017: 12302.50\n",
      "mean_20_dow4_2017: 11594.38\n",
      "mean_20_dow2_2017: 9143.97\n",
      "mean_4_dow2_2017: 7852.31\n",
      "promo_1: 7793.53\n",
      "promo_15: 7516.16\n",
      "mean_4_dow3_2017: 5996.42\n",
      "promo_11: 5861.48\n",
      "mean_4_dow4_2017: 5766.13\n",
      "promo_8: 5093.33\n",
      "promo_10: 3962.17\n",
      "promo_12: 1942.11\n",
      "oil_60_2017: 1665.34\n",
      "oil_0: 0.00\n",
      "oil_1: 0.00\n",
      "oil_2: 0.00\n",
      "oil_3: 0.00\n",
      "oil_4: 0.00\n",
      "oil_5: 0.00\n",
      "oil_6: 0.00\n",
      "oil_7: 0.00\n",
      "oil_8: 0.00\n",
      "oil_9: 0.00\n",
      "oil_10: 0.00\n",
      "oil_11: 0.00\n",
      "oil_12: 0.00\n",
      "oil_13: 0.00\n",
      "oil_14: 0.00\n",
      "oil_15: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Step 8\n",
      "==================================================\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[50]\ttraining's l2: 0.641379\tvalid_1's l2: 0.740505\n",
      "[100]\ttraining's l2: 0.458139\tvalid_1's l2: 0.541669\n",
      "[150]\ttraining's l2: 0.385954\tvalid_1's l2: 0.460492\n",
      "[200]\ttraining's l2: 0.357255\tvalid_1's l2: 0.426958\n",
      "[250]\ttraining's l2: 0.344779\tvalid_1's l2: 0.411907\n",
      "[300]\ttraining's l2: 0.33896\tvalid_1's l2: 0.404817\n",
      "[350]\ttraining's l2: 0.335758\tvalid_1's l2: 0.401185\n",
      "[400]\ttraining's l2: 0.333833\tvalid_1's l2: 0.39947\n",
      "[450]\ttraining's l2: 0.332436\tvalid_1's l2: 0.398683\n",
      "[500]\ttraining's l2: 0.331407\tvalid_1's l2: 0.398267\n",
      "[550]\ttraining's l2: 0.330464\tvalid_1's l2: 0.39797\n",
      "[600]\ttraining's l2: 0.329674\tvalid_1's l2: 0.397773\n",
      "Early stopping, best iteration is:\n",
      "[599]\ttraining's l2: 0.329683\tvalid_1's l2: 0.397766\n",
      "mean_14_2017: 11820813.38\n",
      "mean_30_2017: 8878821.66\n",
      "mean_7_2017: 4292857.23\n",
      "mean_20_dow0_2017: 1471072.51\n",
      "promo_7: 1418225.12\n",
      "mean_60_2017: 1247605.30\n",
      "mean_4_dow0_2017: 815544.83\n",
      "mean_3_2017: 219115.69\n",
      "promo_0: 185448.83\n",
      "promo_14: 145531.30\n",
      "promo_30_2017: 95351.75\n",
      "mean_140_2017: 88270.29\n",
      "day_1_2017: 86134.37\n",
      "promo_14_2017: 74564.64\n",
      "promo_140_2017: 63069.48\n",
      "promo_60_2017: 53781.61\n",
      "mean_4_dow5_2017: 50424.23\n",
      "promo_3: 49869.88\n",
      "promo_7_2017: 45379.19\n",
      "mean_20_dow2_2017: 42713.13\n",
      "promo_6: 39961.18\n",
      "promo_8: 39277.35\n",
      "oil_14_2017: 38649.69\n",
      "mean_20_dow4_2017: 38416.15\n",
      "promo_5: 37781.80\n",
      "oil_140_2017: 28934.74\n",
      "mean_20_dow1_2017: 20167.09\n",
      "mean_4_dow6_2017: 18950.80\n",
      "promo_9: 18493.70\n",
      "mean_4_dow2_2017: 18325.81\n",
      "promo_10: 14551.59\n",
      "promo_2: 13553.42\n",
      "promo_15: 13515.33\n",
      "promo_4: 12422.49\n",
      "mean_20_dow3_2017: 12183.84\n",
      "mean_20_dow6_2017: 11332.86\n",
      "mean_20_dow5_2017: 11284.50\n",
      "oil_7_2017: 10720.88\n",
      "mean_4_dow1_2017: 8513.70\n",
      "promo_11: 8424.67\n",
      "oil_30_2017: 8361.46\n",
      "mean_4_dow4_2017: 7505.35\n",
      "mean_4_dow3_2017: 6002.85\n",
      "promo_12: 5136.96\n",
      "promo_13: 4870.39\n",
      "promo_1: 2137.02\n",
      "oil_60_2017: 1723.85\n",
      "oil_0: 0.00\n",
      "oil_1: 0.00\n",
      "oil_2: 0.00\n",
      "oil_3: 0.00\n",
      "oil_4: 0.00\n",
      "oil_5: 0.00\n",
      "oil_6: 0.00\n",
      "oil_7: 0.00\n",
      "oil_8: 0.00\n",
      "oil_9: 0.00\n",
      "oil_10: 0.00\n",
      "oil_11: 0.00\n",
      "oil_12: 0.00\n",
      "oil_13: 0.00\n",
      "oil_14: 0.00\n",
      "oil_15: 0.00\n",
      "==================================================\n",
      "Step 9\n",
      "==================================================\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[50]\ttraining's l2: 0.595078\tvalid_1's l2: 0.656035\n",
      "[100]\ttraining's l2: 0.443864\tvalid_1's l2: 0.496203\n",
      "[150]\ttraining's l2: 0.385853\tvalid_1's l2: 0.432466\n",
      "[200]\ttraining's l2: 0.362678\tvalid_1's l2: 0.406033\n",
      "[250]\ttraining's l2: 0.352852\tvalid_1's l2: 0.394507\n",
      "[300]\ttraining's l2: 0.348243\tvalid_1's l2: 0.389342\n",
      "[350]\ttraining's l2: 0.34573\tvalid_1's l2: 0.387018\n",
      "[400]\ttraining's l2: 0.344103\tvalid_1's l2: 0.385985\n",
      "[450]\ttraining's l2: 0.342895\tvalid_1's l2: 0.385709\n",
      "[500]\ttraining's l2: 0.341973\tvalid_1's l2: 0.385681\n",
      "[550]\ttraining's l2: 0.341006\tvalid_1's l2: 0.38566\n",
      "Early stopping, best iteration is:\n",
      "[524]\ttraining's l2: 0.341561\tvalid_1's l2: 0.385638\n",
      "mean_30_2017: 10383950.22\n",
      "mean_14_2017: 6827532.75\n",
      "mean_60_2017: 2711311.64\n",
      "mean_7_2017: 2695297.37\n",
      "mean_20_dow1_2017: 1000735.01\n",
      "promo_8: 981391.02\n",
      "mean_4_dow1_2017: 334986.75\n",
      "mean_3_2017: 140562.59\n",
      "mean_140_2017: 113737.79\n",
      "promo_10: 91077.07\n",
      "day_1_2017: 83880.60\n",
      "promo_30_2017: 74753.08\n",
      "promo_14_2017: 68914.21\n",
      "promo_7: 66997.89\n",
      "mean_20_dow2_2017: 61703.86\n",
      "promo_7_2017: 47181.86\n",
      "promo_60_2017: 42445.29\n",
      "promo_12: 35671.52\n",
      "promo_140_2017: 35468.04\n",
      "mean_20_dow4_2017: 35394.63\n",
      "promo_11: 27916.12\n",
      "promo_9: 25371.94\n",
      "mean_4_dow2_2017: 24827.05\n",
      "oil_14_2017: 22338.80\n",
      "mean_20_dow0_2017: 17656.78\n",
      "promo_13: 16280.61\n",
      "oil_140_2017: 15564.52\n",
      "mean_20_dow6_2017: 13768.38\n",
      "mean_4_dow6_2017: 13729.70\n",
      "promo_14: 12716.41\n",
      "promo_0: 11437.13\n",
      "mean_20_dow5_2017: 9658.18\n",
      "mean_4_dow0_2017: 9208.69\n",
      "promo_3: 8743.71\n",
      "mean_4_dow4_2017: 7701.57\n",
      "promo_6: 7450.70\n",
      "oil_7_2017: 7109.23\n",
      "mean_4_dow5_2017: 5468.03\n",
      "mean_20_dow3_2017: 5338.09\n",
      "mean_4_dow3_2017: 5026.39\n",
      "promo_4: 4876.05\n",
      "promo_5: 4491.37\n",
      "oil_30_2017: 3128.24\n",
      "promo_2: 2500.03\n",
      "promo_15: 2349.82\n",
      "promo_1: 1893.85\n",
      "oil_60_2017: 668.88\n",
      "oil_0: 0.00\n",
      "oil_1: 0.00\n",
      "oil_2: 0.00\n",
      "oil_3: 0.00\n",
      "oil_4: 0.00\n",
      "oil_5: 0.00\n",
      "oil_6: 0.00\n",
      "oil_7: 0.00\n",
      "oil_8: 0.00\n",
      "oil_9: 0.00\n",
      "oil_10: 0.00\n",
      "oil_11: 0.00\n",
      "oil_12: 0.00\n",
      "oil_13: 0.00\n",
      "oil_14: 0.00\n",
      "oil_15: 0.00\n",
      "==================================================\n",
      "Step 10\n",
      "==================================================\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[50]\ttraining's l2: 0.649322\tvalid_1's l2: 0.676915\n",
      "[100]\ttraining's l2: 0.47016\tvalid_1's l2: 0.496909\n",
      "[150]\ttraining's l2: 0.400697\tvalid_1's l2: 0.425829\n",
      "[200]\ttraining's l2: 0.372391\tvalid_1's l2: 0.396555\n",
      "[250]\ttraining's l2: 0.360128\tvalid_1's l2: 0.383831\n",
      "[300]\ttraining's l2: 0.354257\tvalid_1's l2: 0.378066\n",
      "[350]\ttraining's l2: 0.351041\tvalid_1's l2: 0.375332\n",
      "[400]\ttraining's l2: 0.348977\tvalid_1's l2: 0.374006\n",
      "[450]\ttraining's l2: 0.347485\tvalid_1's l2: 0.37339\n",
      "[500]\ttraining's l2: 0.34631\tvalid_1's l2: 0.373116\n",
      "[550]\ttraining's l2: 0.345208\tvalid_1's l2: 0.372874\n",
      "[600]\ttraining's l2: 0.344219\tvalid_1's l2: 0.372754\n",
      "Early stopping, best iteration is:\n",
      "[596]\ttraining's l2: 0.344291\tvalid_1's l2: 0.372722\n",
      "mean_30_2017: 9784842.32\n",
      "mean_14_2017: 8181293.10\n",
      "mean_20_dow2_2017: 3549052.11\n",
      "mean_7_2017: 2992821.15\n",
      "mean_4_dow2_2017: 2745883.84\n",
      "mean_60_2017: 1252430.66\n",
      "promo_9: 1117257.66\n",
      "mean_3_2017: 111974.84\n",
      "promo_10: 111700.45\n",
      "promo_30_2017: 108893.59\n",
      "promo_2: 95304.28\n",
      "promo_140_2017: 93750.22\n",
      "promo_14_2017: 91900.96\n",
      "mean_20_dow1_2017: 67633.93\n",
      "promo_8: 67021.46\n",
      "promo_7: 60215.44\n",
      "promo_7_2017: 58190.38\n",
      "mean_20_dow4_2017: 53617.90\n",
      "promo_14: 47523.31\n",
      "promo_60_2017: 44433.46\n",
      "promo_12: 42119.05\n",
      "day_1_2017: 40547.40\n",
      "promo_11: 32914.48\n",
      "mean_4_dow1_2017: 32710.46\n",
      "mean_20_dow0_2017: 31160.29\n",
      "mean_140_2017: 30613.79\n",
      "mean_4_dow3_2017: 27453.57\n",
      "mean_20_dow5_2017: 23714.91\n",
      "promo_13: 20137.82\n",
      "mean_4_dow0_2017: 18487.39\n",
      "oil_140_2017: 18429.43\n",
      "mean_20_dow3_2017: 9288.18\n",
      "mean_20_dow6_2017: 8832.67\n",
      "mean_4_dow4_2017: 7844.46\n",
      "oil_7_2017: 7786.53\n",
      "promo_6: 7328.81\n",
      "mean_4_dow6_2017: 6216.27\n",
      "oil_30_2017: 5721.65\n",
      "promo_4: 5288.55\n",
      "promo_0: 5123.56\n",
      "promo_1: 4804.59\n",
      "mean_4_dow5_2017: 4574.37\n",
      "promo_15: 3999.84\n",
      "oil_14_2017: 3297.22\n",
      "promo_5: 3110.28\n",
      "promo_3: 2157.46\n",
      "oil_60_2017: 717.10\n",
      "oil_0: 0.00\n",
      "oil_1: 0.00\n",
      "oil_2: 0.00\n",
      "oil_3: 0.00\n",
      "oil_4: 0.00\n",
      "oil_5: 0.00\n",
      "oil_6: 0.00\n",
      "oil_7: 0.00\n",
      "oil_8: 0.00\n",
      "oil_9: 0.00\n",
      "oil_10: 0.00\n",
      "oil_11: 0.00\n",
      "oil_12: 0.00\n",
      "oil_13: 0.00\n",
      "oil_14: 0.00\n",
      "oil_15: 0.00\n",
      "==================================================\n",
      "Step 11\n",
      "==================================================\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[50]\ttraining's l2: 0.717339\tvalid_1's l2: 0.695\n",
      "[100]\ttraining's l2: 0.511223\tvalid_1's l2: 0.499669\n",
      "[150]\ttraining's l2: 0.432026\tvalid_1's l2: 0.427043\n",
      "[200]\ttraining's l2: 0.400338\tvalid_1's l2: 0.398895\n",
      "[250]\ttraining's l2: 0.38681\tvalid_1's l2: 0.387283\n",
      "[300]\ttraining's l2: 0.380435\tvalid_1's l2: 0.382127\n",
      "[350]\ttraining's l2: 0.376952\tvalid_1's l2: 0.379711\n",
      "[400]\ttraining's l2: 0.374769\tvalid_1's l2: 0.378439\n",
      "[450]\ttraining's l2: 0.373199\tvalid_1's l2: 0.377781\n",
      "[500]\ttraining's l2: 0.371843\tvalid_1's l2: 0.377343\n",
      "[550]\ttraining's l2: 0.370766\tvalid_1's l2: 0.37703\n",
      "[600]\ttraining's l2: 0.369511\tvalid_1's l2: 0.37697\n",
      "[650]\ttraining's l2: 0.368521\tvalid_1's l2: 0.376986\n",
      "Early stopping, best iteration is:\n",
      "[624]\ttraining's l2: 0.369026\tvalid_1's l2: 0.376909\n",
      "mean_30_2017: 14654989.85\n",
      "mean_14_2017: 7404807.41\n",
      "mean_60_2017: 4144116.37\n",
      "mean_7_2017: 3480584.59\n",
      "mean_4_dow3_2017: 1836515.27\n",
      "mean_20_dow3_2017: 1500714.07\n",
      "promo_10: 876034.78\n",
      "mean_4_dow4_2017: 446843.19\n",
      "mean_3_2017: 190032.12\n",
      "mean_140_2017: 109153.43\n",
      "promo_30_2017: 91653.36\n",
      "oil_140_2017: 79137.19\n",
      "promo_12: 68586.75\n",
      "promo_14: 63729.12\n",
      "mean_4_dow2_2017: 60636.66\n",
      "oil_7_2017: 58229.04\n",
      "promo_140_2017: 56658.15\n",
      "promo_14_2017: 56409.63\n",
      "promo_7_2017: 54326.67\n",
      "promo_11: 53127.19\n",
      "promo_9: 52760.57\n",
      "promo_60_2017: 43052.86\n",
      "promo_7: 42814.93\n",
      "promo_8: 37957.16\n",
      "mean_20_dow2_2017: 30910.36\n",
      "mean_20_dow5_2017: 29565.38\n",
      "promo_13: 28324.69\n",
      "mean_20_dow4_2017: 26980.65\n",
      "day_1_2017: 25387.72\n",
      "mean_20_dow0_2017: 24775.82\n",
      "oil_30_2017: 23838.82\n",
      "oil_14_2017: 20429.47\n",
      "mean_20_dow6_2017: 17464.35\n",
      "promo_3: 14312.52\n",
      "mean_4_dow0_2017: 12921.40\n",
      "mean_4_dow6_2017: 12851.05\n",
      "mean_20_dow1_2017: 9615.74\n",
      "promo_6: 7884.09\n",
      "mean_4_dow1_2017: 6863.59\n",
      "promo_0: 6131.50\n",
      "mean_4_dow5_2017: 5487.06\n",
      "promo_15: 4904.24\n",
      "promo_2: 4642.46\n",
      "promo_4: 2588.41\n",
      "promo_5: 1947.63\n",
      "oil_60_2017: 1488.53\n",
      "promo_1: 1426.53\n",
      "oil_0: 0.00\n",
      "oil_1: 0.00\n",
      "oil_2: 0.00\n",
      "oil_3: 0.00\n",
      "oil_4: 0.00\n",
      "oil_5: 0.00\n",
      "oil_6: 0.00\n",
      "oil_7: 0.00\n",
      "oil_8: 0.00\n",
      "oil_9: 0.00\n",
      "oil_10: 0.00\n",
      "oil_11: 0.00\n",
      "oil_12: 0.00\n",
      "oil_13: 0.00\n",
      "oil_14: 0.00\n",
      "oil_15: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Step 12\n",
      "==================================================\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[50]\ttraining's l2: 0.749654\tvalid_1's l2: 0.72837\n",
      "[100]\ttraining's l2: 0.531579\tvalid_1's l2: 0.519672\n",
      "[150]\ttraining's l2: 0.446672\tvalid_1's l2: 0.441201\n",
      "[200]\ttraining's l2: 0.412275\tvalid_1's l2: 0.411338\n",
      "[250]\ttraining's l2: 0.397498\tvalid_1's l2: 0.399175\n",
      "[300]\ttraining's l2: 0.390488\tvalid_1's l2: 0.393792\n",
      "[350]\ttraining's l2: 0.386656\tvalid_1's l2: 0.391205\n",
      "[400]\ttraining's l2: 0.384208\tvalid_1's l2: 0.389817\n",
      "[450]\ttraining's l2: 0.382468\tvalid_1's l2: 0.388923\n",
      "[500]\ttraining's l2: 0.380992\tvalid_1's l2: 0.388334\n",
      "[550]\ttraining's l2: 0.379711\tvalid_1's l2: 0.387983\n",
      "[600]\ttraining's l2: 0.378374\tvalid_1's l2: 0.387683\n",
      "[650]\ttraining's l2: 0.377315\tvalid_1's l2: 0.387686\n",
      "Early stopping, best iteration is:\n",
      "[613]\ttraining's l2: 0.378048\tvalid_1's l2: 0.387593\n",
      "mean_4_dow4_2017: 16667681.33\n",
      "mean_30_2017: 8731633.48\n",
      "mean_14_2017: 3994539.06\n",
      "mean_20_dow4_2017: 2427960.20\n",
      "mean_60_2017: 2137939.74\n",
      "mean_7_2017: 1108272.73\n",
      "promo_11: 872000.59\n",
      "mean_3_2017: 695812.65\n",
      "promo_12: 120603.10\n",
      "mean_4_dow3_2017: 114582.64\n",
      "mean_140_2017: 101602.05\n",
      "promo_30_2017: 92724.93\n",
      "oil_30_2017: 88870.24\n",
      "promo_14: 84757.71\n",
      "oil_140_2017: 77998.87\n",
      "promo_14_2017: 69382.68\n",
      "promo_10: 55058.45\n",
      "promo_140_2017: 53224.25\n",
      "promo_9: 49234.65\n",
      "oil_7_2017: 47781.78\n",
      "promo_13: 43803.52\n",
      "mean_20_dow3_2017: 42538.58\n",
      "promo_7_2017: 41860.45\n",
      "promo_4: 33369.73\n",
      "promo_7: 33233.15\n",
      "mean_20_dow0_2017: 29489.04\n",
      "promo_60_2017: 27316.85\n",
      "mean_20_dow1_2017: 25925.22\n",
      "promo_8: 24894.87\n",
      "day_1_2017: 23401.57\n",
      "mean_20_dow2_2017: 18404.14\n",
      "oil_14_2017: 18182.37\n",
      "mean_20_dow6_2017: 14128.69\n",
      "mean_20_dow5_2017: 12993.53\n",
      "mean_4_dow0_2017: 9117.25\n",
      "promo_0: 8450.86\n",
      "promo_15: 7548.69\n",
      "promo_6: 6667.11\n",
      "mean_4_dow1_2017: 6410.58\n",
      "mean_4_dow5_2017: 6049.62\n",
      "mean_4_dow6_2017: 5479.18\n",
      "promo_2: 5467.83\n",
      "mean_4_dow2_2017: 4795.38\n",
      "promo_3: 3647.95\n",
      "oil_60_2017: 2199.29\n",
      "promo_5: 1995.74\n",
      "promo_1: 1235.85\n",
      "oil_0: 0.00\n",
      "oil_1: 0.00\n",
      "oil_2: 0.00\n",
      "oil_3: 0.00\n",
      "oil_4: 0.00\n",
      "oil_5: 0.00\n",
      "oil_6: 0.00\n",
      "oil_7: 0.00\n",
      "oil_8: 0.00\n",
      "oil_9: 0.00\n",
      "oil_10: 0.00\n",
      "oil_11: 0.00\n",
      "oil_12: 0.00\n",
      "oil_13: 0.00\n",
      "oil_14: 0.00\n",
      "oil_15: 0.00\n",
      "==================================================\n",
      "Step 13\n",
      "==================================================\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[50]\ttraining's l2: 0.663494\tvalid_1's l2: 0.662835\n",
      "[100]\ttraining's l2: 0.488264\tvalid_1's l2: 0.490511\n",
      "[150]\ttraining's l2: 0.420539\tvalid_1's l2: 0.424586\n",
      "[200]\ttraining's l2: 0.393438\tvalid_1's l2: 0.398756\n",
      "[250]\ttraining's l2: 0.381875\tvalid_1's l2: 0.38797\n",
      "[300]\ttraining's l2: 0.376486\tvalid_1's l2: 0.383241\n",
      "[350]\ttraining's l2: 0.3735\tvalid_1's l2: 0.380904\n",
      "[400]\ttraining's l2: 0.371616\tvalid_1's l2: 0.379676\n",
      "[450]\ttraining's l2: 0.370209\tvalid_1's l2: 0.379068\n",
      "[500]\ttraining's l2: 0.369079\tvalid_1's l2: 0.378719\n",
      "[550]\ttraining's l2: 0.367928\tvalid_1's l2: 0.378415\n",
      "[600]\ttraining's l2: 0.366814\tvalid_1's l2: 0.378224\n",
      "[650]\ttraining's l2: 0.36588\tvalid_1's l2: 0.378204\n",
      "[700]\ttraining's l2: 0.36513\tvalid_1's l2: 0.378193\n",
      "[750]\ttraining's l2: 0.364585\tvalid_1's l2: 0.378143\n",
      "[800]\ttraining's l2: 0.364077\tvalid_1's l2: 0.378137\n",
      "[850]\ttraining's l2: 0.363626\tvalid_1's l2: 0.378109\n",
      "[900]\ttraining's l2: 0.363183\tvalid_1's l2: 0.37809\n",
      "[950]\ttraining's l2: 0.362852\tvalid_1's l2: 0.378081\n",
      "Early stopping, best iteration is:\n",
      "[931]\ttraining's l2: 0.362988\tvalid_1's l2: 0.378068\n",
      "mean_30_2017: 15367554.23\n",
      "mean_14_2017: 5000591.96\n",
      "mean_60_2017: 3920079.02\n",
      "mean_7_2017: 1595921.49\n",
      "promo_12: 925584.25\n",
      "mean_4_dow5_2017: 822612.24\n",
      "mean_20_dow5_2017: 820654.52\n",
      "mean_3_2017: 808042.27\n",
      "mean_140_2017: 116654.03\n",
      "promo_13: 107849.82\n",
      "promo_30_2017: 100209.00\n",
      "promo_14: 95595.82\n",
      "promo_10: 90221.24\n",
      "oil_140_2017: 65052.93\n",
      "promo_14_2017: 64981.87\n",
      "mean_20_dow0_2017: 63641.95\n",
      "mean_20_dow6_2017: 56636.43\n",
      "promo_140_2017: 44323.97\n",
      "oil_7_2017: 41870.82\n",
      "promo_7_2017: 40503.07\n",
      "promo_60_2017: 37660.01\n",
      "day_1_2017: 36054.42\n",
      "promo_11: 33951.03\n",
      "promo_9: 30205.49\n",
      "mean_4_dow6_2017: 29425.70\n",
      "mean_20_dow3_2017: 27013.62\n",
      "promo_7: 21173.15\n",
      "mean_4_dow0_2017: 19536.31\n",
      "promo_15: 19190.47\n",
      "mean_20_dow2_2017: 18928.04\n",
      "promo_8: 18400.86\n",
      "mean_20_dow1_2017: 17999.69\n",
      "oil_14_2017: 16037.29\n",
      "mean_4_dow2_2017: 14836.09\n",
      "mean_20_dow4_2017: 14119.53\n",
      "mean_4_dow4_2017: 14083.01\n",
      "promo_0: 10837.36\n",
      "mean_4_dow3_2017: 10680.98\n",
      "mean_4_dow1_2017: 10660.68\n",
      "promo_5: 9848.02\n",
      "promo_6: 8901.54\n",
      "oil_30_2017: 7681.72\n",
      "promo_2: 6533.55\n",
      "promo_3: 4308.73\n",
      "promo_4: 3471.94\n",
      "promo_1: 1670.86\n",
      "oil_60_2017: 1577.01\n",
      "oil_0: 0.00\n",
      "oil_1: 0.00\n",
      "oil_2: 0.00\n",
      "oil_3: 0.00\n",
      "oil_4: 0.00\n",
      "oil_5: 0.00\n",
      "oil_6: 0.00\n",
      "oil_7: 0.00\n",
      "oil_8: 0.00\n",
      "oil_9: 0.00\n",
      "oil_10: 0.00\n",
      "oil_11: 0.00\n",
      "oil_12: 0.00\n",
      "oil_13: 0.00\n",
      "oil_14: 0.00\n",
      "oil_15: 0.00\n",
      "==================================================\n",
      "Step 14\n",
      "==================================================\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[50]\ttraining's l2: 0.641394\tvalid_1's l2: 0.630933\n",
      "[100]\ttraining's l2: 0.473724\tvalid_1's l2: 0.46867\n",
      "[150]\ttraining's l2: 0.409192\tvalid_1's l2: 0.406995\n",
      "[200]\ttraining's l2: 0.383028\tvalid_1's l2: 0.382977\n",
      "[250]\ttraining's l2: 0.37164\tvalid_1's l2: 0.373108\n",
      "[300]\ttraining's l2: 0.366198\tvalid_1's l2: 0.368867\n",
      "[350]\ttraining's l2: 0.363145\tvalid_1's l2: 0.366857\n",
      "[400]\ttraining's l2: 0.361188\tvalid_1's l2: 0.365811\n",
      "[450]\ttraining's l2: 0.35977\tvalid_1's l2: 0.365285\n",
      "[500]\ttraining's l2: 0.358601\tvalid_1's l2: 0.364891\n",
      "[550]\ttraining's l2: 0.357511\tvalid_1's l2: 0.36473\n",
      "[600]\ttraining's l2: 0.356518\tvalid_1's l2: 0.364755\n",
      "Early stopping, best iteration is:\n",
      "[573]\ttraining's l2: 0.357037\tvalid_1's l2: 0.364679\n",
      "mean_30_2017: 14654412.80\n",
      "mean_14_2017: 4132824.98\n",
      "mean_60_2017: 2709684.49\n",
      "mean_20_dow6_2017: 1826897.12\n",
      "mean_7_2017: 1772602.29\n",
      "promo_13: 1282550.97\n",
      "mean_4_dow6_2017: 853787.78\n",
      "mean_3_2017: 419090.41\n",
      "mean_140_2017: 122419.60\n",
      "promo_14: 117408.84\n",
      "promo_12: 111350.00\n",
      "day_1_2017: 106117.34\n",
      "mean_4_dow5_2017: 96934.64\n",
      "promo_30_2017: 93830.63\n",
      "promo_10: 79732.04\n",
      "mean_20_dow1_2017: 69566.32\n",
      "mean_20_dow5_2017: 63998.63\n",
      "promo_14_2017: 55548.34\n",
      "promo_7_2017: 46693.23\n",
      "promo_6: 43945.10\n",
      "promo_140_2017: 43426.43\n",
      "oil_140_2017: 41678.97\n",
      "mean_20_dow0_2017: 38238.76\n",
      "promo_60_2017: 35345.39\n",
      "oil_7_2017: 28927.20\n",
      "promo_11: 28471.53\n",
      "promo_9: 22437.25\n",
      "promo_7: 17629.21\n",
      "promo_0: 16143.00\n",
      "oil_30_2017: 14766.08\n",
      "mean_4_dow1_2017: 14554.19\n",
      "mean_20_dow3_2017: 14427.97\n",
      "promo_15: 13879.88\n",
      "mean_20_dow4_2017: 13435.32\n",
      "promo_8: 12147.47\n",
      "mean_4_dow0_2017: 11539.88\n",
      "oil_14_2017: 10146.59\n",
      "mean_20_dow2_2017: 8546.14\n",
      "mean_4_dow2_2017: 7091.19\n",
      "promo_2: 6360.58\n",
      "mean_4_dow4_2017: 6257.20\n",
      "mean_4_dow3_2017: 5844.72\n",
      "promo_4: 3035.02\n",
      "promo_3: 2148.06\n",
      "promo_5: 1347.17\n",
      "promo_1: 1201.75\n",
      "oil_60_2017: 1080.43\n",
      "oil_0: 0.00\n",
      "oil_1: 0.00\n",
      "oil_2: 0.00\n",
      "oil_3: 0.00\n",
      "oil_4: 0.00\n",
      "oil_5: 0.00\n",
      "oil_6: 0.00\n",
      "oil_7: 0.00\n",
      "oil_8: 0.00\n",
      "oil_9: 0.00\n",
      "oil_10: 0.00\n",
      "oil_11: 0.00\n",
      "oil_12: 0.00\n",
      "oil_13: 0.00\n",
      "oil_14: 0.00\n",
      "oil_15: 0.00\n",
      "==================================================\n",
      "Step 15\n",
      "==================================================\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[50]\ttraining's l2: 0.648228\tvalid_1's l2: 0.632402\n",
      "[100]\ttraining's l2: 0.470047\tvalid_1's l2: 0.462511\n",
      "[150]\ttraining's l2: 0.400145\tvalid_1's l2: 0.396661\n",
      "[200]\ttraining's l2: 0.37182\tvalid_1's l2: 0.370635\n",
      "[250]\ttraining's l2: 0.359734\tvalid_1's l2: 0.359959\n",
      "[300]\ttraining's l2: 0.353875\tvalid_1's l2: 0.3552\n",
      "[350]\ttraining's l2: 0.350788\tvalid_1's l2: 0.353045\n",
      "[400]\ttraining's l2: 0.348646\tvalid_1's l2: 0.351696\n",
      "[450]\ttraining's l2: 0.347188\tvalid_1's l2: 0.350949\n",
      "[500]\ttraining's l2: 0.346012\tvalid_1's l2: 0.350448\n",
      "[550]\ttraining's l2: 0.345022\tvalid_1's l2: 0.350207\n",
      "[600]\ttraining's l2: 0.344136\tvalid_1's l2: 0.350028\n",
      "[650]\ttraining's l2: 0.343312\tvalid_1's l2: 0.349907\n",
      "[700]\ttraining's l2: 0.342712\tvalid_1's l2: 0.349895\n",
      "[750]\ttraining's l2: 0.342128\tvalid_1's l2: 0.349796\n",
      "[800]\ttraining's l2: 0.341601\tvalid_1's l2: 0.349754\n",
      "Early stopping, best iteration is:\n",
      "[795]\ttraining's l2: 0.341651\tvalid_1's l2: 0.349734\n",
      "mean_30_2017: 14616463.84\n",
      "mean_14_2017: 5674516.01\n",
      "mean_20_dow0_2017: 2258452.48\n",
      "mean_7_2017: 2150057.79\n",
      "mean_60_2017: 1922703.33\n",
      "promo_14: 1659456.97\n",
      "mean_4_dow0_2017: 763746.09\n",
      "mean_140_2017: 152095.79\n",
      "promo_7: 151690.71\n",
      "mean_3_2017: 144422.21\n",
      "promo_30_2017: 123265.33\n",
      "promo_13: 92457.10\n",
      "promo_0: 91120.55\n",
      "promo_140_2017: 87880.31\n",
      "promo_12: 76271.43\n",
      "mean_20_dow2_2017: 71858.52\n",
      "promo_14_2017: 65478.59\n",
      "day_1_2017: 62970.77\n",
      "promo_7_2017: 57048.16\n",
      "promo_60_2017: 55503.37\n",
      "mean_4_dow5_2017: 53785.19\n",
      "promo_15: 52035.80\n",
      "mean_20_dow4_2017: 46477.97\n",
      "promo_10: 44453.41\n",
      "mean_20_dow1_2017: 38734.60\n",
      "oil_140_2017: 30047.76\n",
      "promo_9: 29755.86\n",
      "mean_4_dow2_2017: 22440.26\n",
      "mean_4_dow6_2017: 21696.78\n",
      "mean_20_dow6_2017: 18569.77\n",
      "promo_11: 18216.03\n",
      "oil_7_2017: 17758.63\n",
      "mean_20_dow3_2017: 16559.50\n",
      "promo_2: 14987.73\n",
      "mean_20_dow5_2017: 13257.07\n",
      "mean_4_dow1_2017: 11240.69\n",
      "promo_6: 10153.25\n",
      "mean_4_dow4_2017: 10011.85\n",
      "promo_8: 8168.68\n",
      "mean_4_dow3_2017: 8035.45\n",
      "oil_14_2017: 7380.63\n",
      "promo_4: 7274.96\n",
      "oil_30_2017: 5506.50\n",
      "promo_3: 4588.79\n",
      "promo_5: 4057.88\n",
      "promo_1: 3278.15\n",
      "oil_60_2017: 727.33\n",
      "oil_0: 0.00\n",
      "oil_1: 0.00\n",
      "oil_2: 0.00\n",
      "oil_3: 0.00\n",
      "oil_4: 0.00\n",
      "oil_5: 0.00\n",
      "oil_6: 0.00\n",
      "oil_7: 0.00\n",
      "oil_8: 0.00\n",
      "oil_9: 0.00\n",
      "oil_10: 0.00\n",
      "oil_11: 0.00\n",
      "oil_12: 0.00\n",
      "oil_13: 0.00\n",
      "oil_14: 0.00\n",
      "oil_15: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Step 16\n",
      "==================================================\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[50]\ttraining's l2: 0.605965\tvalid_1's l2: 0.615118\n",
      "[100]\ttraining's l2: 0.45803\tvalid_1's l2: 0.470145\n",
      "[150]\ttraining's l2: 0.400805\tvalid_1's l2: 0.414015\n",
      "[200]\ttraining's l2: 0.377832\tvalid_1's l2: 0.391422\n",
      "[250]\ttraining's l2: 0.368047\tvalid_1's l2: 0.381898\n",
      "[300]\ttraining's l2: 0.363531\tvalid_1's l2: 0.377714\n",
      "[350]\ttraining's l2: 0.361008\tvalid_1's l2: 0.375652\n",
      "[400]\ttraining's l2: 0.359336\tvalid_1's l2: 0.374549\n",
      "[450]\ttraining's l2: 0.358008\tvalid_1's l2: 0.373767\n",
      "[500]\ttraining's l2: 0.356996\tvalid_1's l2: 0.373349\n",
      "[550]\ttraining's l2: 0.356035\tvalid_1's l2: 0.373149\n",
      "[600]\ttraining's l2: 0.355028\tvalid_1's l2: 0.372937\n",
      "[650]\ttraining's l2: 0.354236\tvalid_1's l2: 0.372797\n",
      "[700]\ttraining's l2: 0.353603\tvalid_1's l2: 0.37265\n",
      "[750]\ttraining's l2: 0.353112\tvalid_1's l2: 0.372578\n",
      "[800]\ttraining's l2: 0.352601\tvalid_1's l2: 0.372463\n",
      "[850]\ttraining's l2: 0.352179\tvalid_1's l2: 0.372364\n",
      "[900]\ttraining's l2: 0.351785\tvalid_1's l2: 0.372319\n",
      "[950]\ttraining's l2: 0.351431\tvalid_1's l2: 0.372253\n",
      "[1000]\ttraining's l2: 0.351091\tvalid_1's l2: 0.372191\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's l2: 0.351091\tvalid_1's l2: 0.372191\n",
      "mean_30_2017: 13501757.90\n",
      "mean_14_2017: 3935783.11\n",
      "mean_60_2017: 3501232.44\n",
      "mean_7_2017: 1119659.70\n",
      "promo_15: 1109033.28\n",
      "mean_20_dow1_2017: 1104700.43\n",
      "mean_140_2017: 351229.24\n",
      "mean_4_dow1_2017: 131081.91\n",
      "mean_20_dow2_2017: 107974.50\n",
      "promo_14: 101642.21\n",
      "promo_30_2017: 93784.68\n",
      "mean_3_2017: 91080.49\n",
      "day_1_2017: 67591.86\n",
      "promo_140_2017: 60829.65\n",
      "promo_60_2017: 57081.75\n",
      "mean_4_dow6_2017: 56713.62\n",
      "promo_14_2017: 55686.11\n",
      "mean_20_dow4_2017: 47756.75\n",
      "oil_140_2017: 34789.87\n",
      "promo_7_2017: 34017.20\n",
      "promo_13: 33935.78\n",
      "mean_20_dow0_2017: 33333.14\n",
      "promo_10: 32079.34\n",
      "mean_4_dow2_2017: 27083.83\n",
      "promo_12: 25746.08\n",
      "mean_4_dow0_2017: 20815.40\n",
      "mean_20_dow6_2017: 20567.81\n",
      "promo_7: 19275.56\n",
      "oil_14_2017: 16222.42\n",
      "oil_30_2017: 15226.14\n",
      "promo_8: 14411.14\n",
      "mean_20_dow5_2017: 14198.39\n",
      "mean_4_dow4_2017: 13112.17\n",
      "mean_20_dow3_2017: 11289.92\n",
      "promo_9: 10624.54\n",
      "mean_4_dow5_2017: 10490.52\n",
      "mean_4_dow3_2017: 10388.80\n",
      "promo_0: 10312.83\n",
      "promo_11: 9493.77\n",
      "oil_7_2017: 9050.64\n",
      "promo_6: 8573.99\n",
      "promo_3: 5595.44\n",
      "promo_2: 4804.00\n",
      "promo_4: 4654.99\n",
      "promo_1: 4174.09\n",
      "promo_5: 3625.68\n",
      "oil_60_2017: 1245.05\n",
      "oil_0: 0.00\n",
      "oil_1: 0.00\n",
      "oil_2: 0.00\n",
      "oil_3: 0.00\n",
      "oil_4: 0.00\n",
      "oil_5: 0.00\n",
      "oil_6: 0.00\n",
      "oil_7: 0.00\n",
      "oil_8: 0.00\n",
      "oil_9: 0.00\n",
      "oil_10: 0.00\n",
      "oil_11: 0.00\n",
      "oil_12: 0.00\n",
      "oil_13: 0.00\n",
      "oil_14: 0.00\n",
      "oil_15: 0.00\n",
      "Validation mse: 0.365812379375\n",
      "Making submission...\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "print(\"Training and predicting models...\")\n",
    "params = {\n",
    "    'num_leaves': 80,\n",
    "    'objective': 'regression_l2',\n",
    "    'max_depth': 8,\n",
    "    'min_data_in_leaf': 500,\n",
    "    'learning_rate': 0.01,\n",
    "    'feature_fraction': 0.75,\n",
    "    'bagging_fraction': 0.75,\n",
    "    'bagging_freq': 1,\n",
    "    'metric': 'l2',\n",
    "    'num_threads': 4,\n",
    "    'subsample': 0.8,\n",
    "     'random_state': 42,\n",
    "    'min_samples_split':500,\n",
    "    'min_samples_leaf':50,\n",
    "    'max_features': 'sqrt',\n",
    "    'n_estimators':600,\n",
    "    'warm_start':'True'\n",
    "}\n",
    "\n",
    "MAX_ROUNDS = 1000\n",
    "val_pred = []\n",
    "test_pred = []\n",
    "cate_vars = []\n",
    "for i in range(16):\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Step %d\" % (i+1))\n",
    "    print(\"=\" * 50)\n",
    "    dtrain = lgb.Dataset(\n",
    "        X_train, label=y_train[:, i],\n",
    "        categorical_feature=cate_vars,\n",
    "        weight=pd.concat([items[\"perishable\"]] * 6) * 0.25 + 1\n",
    "    )\n",
    "    dval = lgb.Dataset(\n",
    "        X_val, label=y_val[:, i], reference=dtrain,\n",
    "        weight=items[\"perishable\"] * 0.25 + 1,\n",
    "        categorical_feature=cate_vars)\n",
    "    bst = lgb.train(\n",
    "        params, dtrain, num_boost_round=MAX_ROUNDS,\n",
    "        valid_sets=[dtrain, dval], early_stopping_rounds=50, verbose_eval=50\n",
    "    )\n",
    "    print(\"\\n\".join((\"%s: %.2f\" % x) for x in sorted(\n",
    "        zip(X_train.columns, bst.feature_importance(\"gain\")),\n",
    "        key=lambda x: x[1], reverse=True\n",
    "    )))\n",
    "    val_pred.append(bst.predict(\n",
    "        X_val, num_iteration=bst.best_iteration or MAX_ROUNDS))\n",
    "    test_pred.append(bst.predict(\n",
    "        X_test, num_iteration=bst.best_iteration or MAX_ROUNDS))\n",
    "\n",
    "print(\"Validation mse:\", mean_squared_error(\n",
    "    y_val, np.array(val_pred).transpose()))\n",
    "\n",
    "print(\"Making submission...\")\n",
    "y_test = np.array(test_pred).transpose()\n",
    "df_preds = pd.DataFrame(\n",
    "    y_test, index=df_2017.index,\n",
    "    columns=pd.date_range(\"2017-08-16\", periods=16)\n",
    ").stack().to_frame(\"unit_sales\")\n",
    "df_preds.index.set_names([\"store_nbr\", \"item_nbr\", \"date\"], inplace=True)\n",
    "\n",
    "submission = df_test[[\"id\"]].join(df_preds, how=\"left\").fillna(0)\n",
    "submission[\"unit_sales\"] = np.clip(np.expm1(submission[\"unit_sales\"]), 0, 1000)\n",
    "submission.to_csv('lgb.csv', float_format='%.4f', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "   ...: import xgboost as xgb\n",
    "   ...: import numpy as np\n",
    "   ...: from sklearn.preprocessing import StandardScaler\n",
    "   ...: from sklearn.pipeline import Pipeline\n",
    "   ...: from sklearn.model_selection import RandomizedSearchCV\n",
    "In [2]: names = [\"crime\",\"zone\",\"industry\",\"charles\",\"no\",\n",
    "   ...: \"rooms\",\"age\", \"distance\",\"radial\",\"tax\",\n",
    "   ...: \"pupil\",\"aam\",\"lower\",\"med_price\"]\n",
    "In [3]: data = pd.read_csv(\"boston_housing.csv\",names=names)\n",
    "In [4]: X, y = data.iloc[:,:-1],data.iloc[:,-1]\n",
    "In [5]: xgb_pipeline = Pipeline[(\"st_scaler\",       \n",
    "   ...: StandardScaler()), (\"xgb_model\",xgb.XGBRegressor())]\n",
    "In [6]: gbm_param_grid = {\n",
    "   ...:     'xgb_model__subsample': np.arange(.05, 1, .05),\n",
    "   ...:     'xgb_model__max_depth': np.arange(3,20,1),\n",
    "   ...:     'xgb_model__colsample_bytree': np.arange(.1,1.05,.05) }\n",
    "In [7]: randomized_neg_mse = RandomizedSearchCV(estimator=xgb_pipeline,\n",
    "   ...: param_distributions=gbm_param_grid, n_iter=10,\n",
    "   ...: scoring='neg_mean_squared_error', cv=4)\n",
    "    In [9]: print(\"Best rmse: \",\n",
    "   ...: np.sqrt(np.abs(randomized_neg_mse.best_score_)))\n",
    "Best rmse: 3.9966784203040677\n",
    "In [10]: print(\"Best model: \",\n",
    "    ...: randomized_neg_mse.best_estimator_)\n",
    "Best model:  Pipeline(steps=[('st_scaler', StandardScaler(copy=True, \n",
    "with_mean=True, with_std=True)),\n",
    "('xgb_model', XGBRegressor(base_score=0.5, colsample_bylevel=1,\n",
    "       colsample_bytree=0.95000000000000029, gamma=0, learning_rate=0.1,\n",
    "       max_delta_step=0, max_depth=8, min_child_weight=1, missing=None,\n",
    "       n_estimators=100, nthread=-1, objective='reg:linear', reg_alpha=0,\n",
    "       reg_lambda=1, scale_pos_weight=1, seed=0, silent=True,\n",
    "       subsample=0.90000000000000013))])\n",
    "    \n",
    "    gbm_param_grid = {\n",
    "    'clf__learning_rate': np.arange(.05, 1, .05),\n",
    "    'clf__max_depth': np.arange(3,10, 1),\n",
    "    'clf__n_estimators': np.arange(50, 200, 50)\n",
    "}\n",
    "\n",
    "# Perform RandomizedSearchCV\n",
    "randomized_roc_auc = RandomizedSearchCV(estimator=pipeline,\n",
    "                                        param_distributions=gbm_param_grid,\n",
    "                                        n_iter=2, scoring='roc_auc', cv=2, verbose=1)\n",
    "\n",
    "# Fit the estimator\n",
    "randomized_roc_auc.fit(X, y)\n",
    "\n",
    "# Compute metrics\n",
    "print(randomized_roc_auc.best_score_)\n",
    "print(randomized_roc_auc.best_estimator_)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
